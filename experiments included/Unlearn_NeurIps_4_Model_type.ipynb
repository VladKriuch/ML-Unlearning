{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAdoaexTwNE_",
        "outputId": "9d7a4b38-cd84-4678-8ca3-10972eacdfe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: CUDA\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Imports\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model, model_selection\n",
        "from tempfile import TemporaryDirectory\n",
        "import seaborn as sns\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models import resnet18\n",
        "import numpy as np\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on device:\", DEVICE.upper())\n",
        "\n",
        "# manual random seed is used for dataset partitioning\n",
        "# to ensure reproducible results across runs\n",
        "RNG = torch.Generator().manual_seed(1991)\n",
        "torch.manual_seed(1991)\n",
        "torch.cuda.manual_seed(1991)\n",
        "\n",
        "import random\n",
        "random.seed(1991)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1991)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR 10 dataset\n",
        "torch.manual_seed(1991)\n",
        "# Transformations\n",
        "normalize = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train data\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=normalize\n",
        ")\n",
        "# Train loader\n",
        "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "# Test data\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=normalize\n",
        ")\n",
        "# Test loader\n",
        "test_loader = DataLoader(test_set, batch_size=256, shuffle=True, num_workers=2)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P6jGCBiwRjk",
        "outputId": "d997b612-954a-4bc7-c15e-ea5201c13662"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(weights=None, num_classes=10)\n",
        "for k, m in enumerate(model.modules()):\n",
        "  print(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEhuoUdbw4Lz",
        "outputId": "3455a0a3-63dc-4912-c9b8-7a11a51cd99b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "ReLU(inplace=True)\n",
            "MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "BasicBlock(\n",
            "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "ReLU(inplace=True)\n",
            "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "BasicBlock(\n",
            "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "ReLU(inplace=True)\n",
            "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "BasicBlock(\n",
            "  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (downsample): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "ReLU(inplace=True)\n",
            "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "Sequential(\n",
            "  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "BasicBlock(\n",
            "  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "ReLU(inplace=True)\n",
            "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "BasicBlock(\n",
            "  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (downsample): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "ReLU(inplace=True)\n",
            "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "Sequential(\n",
            "  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "BasicBlock(\n",
            "  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "ReLU(inplace=True)\n",
            "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "BasicBlock(\n",
            "  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (downsample): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "ReLU(inplace=True)\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "Sequential(\n",
            "  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "BasicBlock(\n",
            "  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "ReLU(inplace=True)\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "Linear(in_features=512, out_features=10, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.prune as prune\n",
        "from math import sqrt\n",
        "import json\n",
        "from copy import deepcopy\n",
        "\n",
        "def kl_loss_fn(outputs, dist_target):\n",
        "    kl_loss = F.kl_div(torch.log_softmax(outputs, dim=1), dist_target, log_target=True, reduction='batchmean')\n",
        "    return kl_loss\n",
        "\n",
        "def entropy_loss_fn(outputs, labels, dist_target, class_weights):\n",
        "    ce_loss = F.cross_entropy(outputs, labels, weight=class_weights)\n",
        "    entropy_dist_target = torch.sum(-torch.exp(dist_target) * dist_target, dim=1)\n",
        "    entropy_outputs = torch.sum(-torch.softmax(outputs, dim=1) * torch.log_softmax(outputs, dim=1), dim=1)\n",
        "    entropy_loss = F.mse_loss(entropy_outputs, entropy_dist_target)\n",
        "    return ce_loss + entropy_loss\n",
        "\n",
        "\n",
        "def unlearning(\n",
        "    net,\n",
        "    retain_loader,\n",
        "    forget_loader,\n",
        "    val_loader,\n",
        "    class_weights=None,\n",
        "):\n",
        "    \"\"\"Simple unlearning by finetuning.\"\"\"\n",
        "    epochs = 3.2\n",
        "    max_iters = int(len(retain_loader) * epochs)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.0005,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    initial_net = deepcopy(net)\n",
        "\n",
        "    net.train()\n",
        "    initial_net.eval()\n",
        "\n",
        "    def prune_model(net, amount=0.95, rand_init=True):\n",
        "        # Modules to prune\n",
        "        modules = list()\n",
        "        for k, m in enumerate(net.modules()):\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                modules.append((m, 'weight'))\n",
        "                if m.bias is not None:\n",
        "                    modules.append((m, 'bias'))\n",
        "\n",
        "        # Prune criteria\n",
        "        prune.global_unstructured(\n",
        "            modules,\n",
        "            #pruning_method=prune.RandomUnstructured,\n",
        "            pruning_method=prune.L1Unstructured,\n",
        "            amount=amount,\n",
        "        )\n",
        "\n",
        "        # Perform the prune\n",
        "        for k, m in enumerate(net.modules()):\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                prune.remove(m, 'weight')\n",
        "                if m.bias is not None:\n",
        "                    prune.remove(m, 'bias')\n",
        "\n",
        "        # Random initialization\n",
        "        if rand_init:\n",
        "            for k, m in enumerate(net.modules()):\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    mask = m.weight == 0\n",
        "                    c_in = mask.shape[1]\n",
        "                    k = 1/(c_in*mask.shape[2]*mask.shape[3])\n",
        "                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n",
        "                    m.weight.data[mask] = randinit[mask]\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    mask = m.weight == 0\n",
        "                    c_in = mask.shape[1]\n",
        "                    k = 1/c_in\n",
        "                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n",
        "                    m.weight.data[mask] = randinit[mask]\n",
        "\n",
        "    num_iters = 0\n",
        "    running = True\n",
        "    prune_amount = 0.99\n",
        "    prune_model(net, prune_amount, True)\n",
        "    while running:\n",
        "        net.train()\n",
        "        for sample in retain_loader:\n",
        "            inputs = sample[0]\n",
        "            targets = sample[1]\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "            # Get target distribution\n",
        "            with torch.no_grad():\n",
        "                original_outputs = initial_net(inputs)\n",
        "                preds = torch.log_softmax(original_outputs, dim=1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = entropy_loss_fn(outputs, targets, preds, class_weights)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            num_iters += 1\n",
        "            # Stop at max iters\n",
        "            if num_iters > max_iters:\n",
        "                running = False\n",
        "                break\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    return net\n",
        ""
      ],
      "metadata": {
        "id": "pCa9me6qwU72"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def train_mia_on_class(model, class_index, train_set, test_set, no_class=True, splits_num=5):\n",
        "  model = model.to(\"cpu\")\n",
        "\n",
        "  if not no_class:\n",
        "    # Select indencies where class is class_index\n",
        "    train_class_set = np.where(np.array(train_set.targets) == class_index)[0]\n",
        "    test_class_set = np.where(np.array(test_set.targets) == class_index)[0]\n",
        "  else:\n",
        "    train_class_set = np.arange(len(train_set))\n",
        "    test_class_set = np.arange(len(test_set))\n",
        "\n",
        "  max_len = min([len(train_class_set), len(test_class_set)])\n",
        "  # Make equal sizes\n",
        "  train_class_set = train_class_set[:max_len]\n",
        "  test_class_set = test_class_set[:max_len]\n",
        "\n",
        "  # Obtain subsets\n",
        "  train_class_set = torch.utils.data.Subset(train_set, train_class_set)\n",
        "  test_class_set = torch.utils.data.Subset(test_set, test_class_set)\n",
        "\n",
        "\n",
        "  # Make them\n",
        "  class_test_loader = torch.utils.data.DataLoader(\n",
        "    test_class_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "\n",
        "  class_train_loader = torch.utils.data.DataLoader(\n",
        "    train_class_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "  # Obtain train and test logits\n",
        "  logits_train = []\n",
        "  for i, (images, labels) in enumerate(class_train_loader, 0):\n",
        "    for img, label in zip(images, labels):\n",
        "      logits = model(img.unsqueeze(0))\n",
        "      losses = criterion(logits, label.unsqueeze(0)).numpy(force=True)\n",
        "\n",
        "      logits_train.append(np.concatenate((logits.detach().numpy()[0], losses)))\n",
        "\n",
        "  logits_test = []\n",
        "  for i, (images, labels) in enumerate(class_test_loader, 0):\n",
        "    for img, label in zip(images, labels):\n",
        "      logits = model(img.unsqueeze(0))\n",
        "      losses = criterion(logits, label.unsqueeze(0)).numpy(force=True)\n",
        "\n",
        "      logits_test.append((np.concatenate((logits.detach().numpy()[0], losses))))\n",
        "\n",
        "  logits_train, logits_test = np.array(logits_train), np.array(logits_test)\n",
        "  # Create dataset\n",
        "  ys = [1] * max_len + [0] * max_len\n",
        "  ys = np.array(ys)\n",
        "  p = np.random.permutation(len(ys))\n",
        "  logits = np.concatenate((logits_train, logits_test))\n",
        "\n",
        "  logits = logits[p]\n",
        "  ys = ys[p]\n",
        "\n",
        "  # Fit logitstic regression\n",
        "  clf = LogisticRegression(random_state=0, max_iter=1000)\n",
        "  cv = model_selection.StratifiedShuffleSplit(\n",
        "        n_splits=splits_num, random_state=0\n",
        "    )\n",
        "\n",
        "  clf = model_selection.cross_val_score(\n",
        "        clf, logits, ys, cv=cv, scoring=\"accuracy\"\n",
        "    )\n",
        "\n",
        "  # Output score\n",
        "  # print(f\"Mean MIA attack score for class {class_index} is: \" + str(clf.mean()))\n",
        "  return clf.mean()\n",
        ""
      ],
      "metadata": {
        "id": "yrq03hBnynXz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Check accuracy\n",
        "\n",
        "def accuracy(net, loader):\n",
        "    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (inputs, targets) in enumerate(loader, 0):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        ""
      ],
      "metadata": {
        "id": "zA-0WjeryqGf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_losses(net, loader):\n",
        "    \"\"\"Auxiliary function to compute per-sample losses\"\"\"\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    all_losses = []\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        logits = net(inputs)\n",
        "        losses = criterion(logits, targets).numpy(force=True)\n",
        "        for l in losses:\n",
        "            all_losses.append(l)\n",
        "\n",
        "    return np.array(all_losses)"
      ],
      "metadata": {
        "id": "r_mpVWQyyrDT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline"
      ],
      "metadata": {
        "id": "XS36-v_LytRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"----- 5 PERCENT -------\")\n",
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "# Model parameters\n",
        "model_name = \"Resnet18 Baseline\"\n",
        "model_params = \"cifar10_resnet18_baseline.pt\"\n",
        "\n",
        "# Percantage of class to forget\n",
        "amount = 0.05\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "# mia scores\n",
        "MIA_scores_before = []\n",
        "MIA_scores = []\n",
        "\n",
        "# Accuracy\n",
        "Accuracy_forget_before = []\n",
        "\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "\n",
        "\n",
        "print(f\"Model name: {model_name}\")\n",
        "print(f\"Amount: {100 * amount}\")\n",
        "\n",
        "for class_num in classes:\n",
        "  print(f\"------ UNLEARNING CLASS {class_num} ------\\n\")\n",
        "  # Define model from trained params\n",
        "\n",
        "  model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "  model_forget_ft.load_state_dict(torch.load(model_params))\n",
        "  model_forget_ft.eval()\n",
        "  model_forget_ft = model_forget_ft.to(DEVICE)\n",
        "\n",
        "  # --- FORMING DATASET ------\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == class_num)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # ---------\n",
        "\n",
        "  # --- EVALUATING MODEL BEFORE ----\n",
        "  accuracy_forget = 100.0 * accuracy(model_forget_ft, forget_loader)\n",
        "  Accuracy_forget_before.append(accuracy_forget)\n",
        "  print(f\"Accuracy before on forget set: {accuracy_forget}\")\n",
        "\n",
        "  mia_attack = train_mia_on_class(model_forget_ft, class_num, forget_set, test_set, True, 2)\n",
        "  MIA_scores_before.append(mia_attack)\n",
        "  print(f\"MIA score before on forget set: {np.round(mia_attack, 3)}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  model_forget_ft = model_forget_ft.to(DEVICE)\n",
        "  # --------------\n",
        "\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning(model_forget_ft, retain_loader, forget_loader, test_loader)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "  print()\n",
        "  model_ft_forget.eval()\n",
        "  mia_scores_fr = train_mia_on_class(model_forget_ft, class_num, forget_set, test_set, True, 5)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr:.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "  print()\n",
        "  print('-' * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vaq5tNQGPli",
        "outputId": "52723219-a44b-4dec-dd3c-071918738586"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- 5 PERCENT -------\n",
            "Model name: Resnet18 Baseline\n",
            "Amount: 5.0\n",
            "------ UNLEARNING CLASS 0 ------\n",
            "\n",
            "Accuracy before on forget set: 97.6\n",
            "MIA score before on forget set: 0.94\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 68.1%\n",
            "Test set accuracy FORGET model: 58.2%\n",
            "Forget set accuracy FORGET model: 52.8%\n",
            "\n",
            "The MIA has an accuracy of 0.856 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 1 ------\n",
            "\n",
            "Accuracy before on forget set: 96.8\n",
            "MIA score before on forget set: 0.95\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 68.6%\n",
            "Test set accuracy FORGET model: 57.5%\n",
            "Forget set accuracy FORGET model: 57.6%\n",
            "\n",
            "The MIA has an accuracy of 0.896 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 2 ------\n",
            "\n",
            "Accuracy before on forget set: 93.2\n",
            "MIA score before on forget set: 0.89\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 67.5%\n",
            "Test set accuracy FORGET model: 57.2%\n",
            "Forget set accuracy FORGET model: 56.4%\n",
            "\n",
            "The MIA has an accuracy of 0.752 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 3 ------\n",
            "\n",
            "Accuracy before on forget set: 85.2\n",
            "MIA score before on forget set: 0.85\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 67.8%\n",
            "Test set accuracy FORGET model: 56.3%\n",
            "Forget set accuracy FORGET model: 28.8%\n",
            "\n",
            "The MIA has an accuracy of 0.792 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 4 ------\n",
            "\n",
            "Accuracy before on forget set: 94.8\n",
            "MIA score before on forget set: 0.9\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 67.0%\n",
            "Test set accuracy FORGET model: 56.3%\n",
            "Forget set accuracy FORGET model: 48.4%\n",
            "\n",
            "The MIA has an accuracy of 0.832 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 5 ------\n",
            "\n",
            "Accuracy before on forget set: 88.4\n",
            "MIA score before on forget set: 0.87\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 67.7%\n",
            "Test set accuracy FORGET model: 57.0%\n",
            "Forget set accuracy FORGET model: 51.6%\n",
            "\n",
            "The MIA has an accuracy of 0.808 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 6 ------\n",
            "\n",
            "Accuracy before on forget set: 96.8\n",
            "MIA score before on forget set: 0.93\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 64.0%\n",
            "Test set accuracy FORGET model: 55.3%\n",
            "Forget set accuracy FORGET model: 79.6%\n",
            "\n",
            "The MIA has an accuracy of 0.848 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 7 ------\n",
            "\n",
            "Accuracy before on forget set: 95.19999999999999\n",
            "MIA score before on forget set: 0.95\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 63.4%\n",
            "Test set accuracy FORGET model: 53.0%\n",
            "Forget set accuracy FORGET model: 50.8%\n",
            "\n",
            "The MIA has an accuracy of 0.828 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 8 ------\n",
            "\n",
            "Accuracy before on forget set: 98.4\n",
            "MIA score before on forget set: 0.91\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 66.6%\n",
            "Test set accuracy FORGET model: 56.4%\n",
            "Forget set accuracy FORGET model: 72.4%\n",
            "\n",
            "The MIA has an accuracy of 0.820 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 9 ------\n",
            "\n",
            "Accuracy before on forget set: 95.6\n",
            "MIA score before on forget set: 0.89\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 66.7%\n",
            "Test set accuracy FORGET model: 55.2%\n",
            "Forget set accuracy FORGET model: 72.4%\n",
            "\n",
            "The MIA has an accuracy of 0.804 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# mia scores\n",
        "print(MIA_scores_before)\n",
        "print(MIA_scores)\n",
        "\n",
        "# Accuracy\n",
        "print(Accuracy_forget_before)\n",
        "\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miT4aP1UGYKX",
        "outputId": "ff37f944-99b1-4369-94cc-3dc09210cf5e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.94, 0.95, 0.89, 0.85, 0.9, 0.87, 0.9299999999999999, 0.95, 0.91, 0.89]\n",
            "[0.8560000000000001, 0.8960000000000001, 0.752, 0.792, 0.8320000000000001, 0.808, 0.8480000000000001, 0.8280000000000001, 0.82, 0.804]\n",
            "[97.6, 96.8, 93.2, 85.2, 94.8, 88.4, 96.8, 95.19999999999999, 98.4, 95.6]\n",
            "[68.08891211286854, 68.57527584058525, 67.53285903774268, 67.76605366294845, 67.00164810869478, 67.66627138062027, 64.02701290348514, 63.402134373053045, 66.63317188850259, 66.67872575620541]\n",
            "[52.800000000000004, 57.599999999999994, 56.39999999999999, 28.799999999999997, 48.4, 51.6, 79.60000000000001, 50.8, 72.39999999999999, 72.39999999999999]\n",
            "[58.160000000000004, 57.48, 57.16, 56.34, 56.330000000000005, 56.98, 55.26, 52.980000000000004, 56.44, 55.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mean_mia_score_before_baseline = np.mean(np.array(MIA_scores_before))\n",
        "mean_mia_score_after_baseline = np.mean(np.array(MIA_scores))\n",
        "\n",
        "mean_accuracy_forget_before_baseline = np.mean(np.array(Accuracy_forget_before))\n",
        "\n",
        "mean_accuracy_forget_after_baseline = np.mean(np.array(Accuracy_forget))\n",
        "mean_accuracy_retain_baseline = np.mean(np.array(Accuracy_retain))\n",
        "mean_accuracy_test_baseline = np.mean(np.array(Accuracy_test))"
      ],
      "metadata": {
        "id": "84SQicWfGa43"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(mean_mia_score_before_baseline)\n",
        "print(mean_mia_score_after_baseline)\n",
        "\n",
        "print(mean_accuracy_forget_before_baseline)\n",
        "\n",
        "print(mean_accuracy_forget_after_baseline)\n",
        "print(mean_accuracy_retain_baseline)\n",
        "print(mean_accuracy_test_baseline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV1Xqj2oGcIr",
        "outputId": "78fc5636-16ea-46f5-9f92-fc0a2c88ab38"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.908\n",
            "0.8236000000000001\n",
            "94.2\n",
            "57.08\n",
            "66.73720650647061\n",
            "56.233000000000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropout model"
      ],
      "metadata": {
        "id": "QKKqZraZGfAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_model(model_train_reg):\n",
        "  from collections import OrderedDict\n",
        "\n",
        "  layer1 = model_train_reg.layer1\n",
        "\n",
        "  for indx in range(len(layer1)):\n",
        "    modified_layers = OrderedDict()\n",
        "    for name, feature in layer1[indx].named_children():\n",
        "      modified_layers[name] = feature\n",
        "      if isinstance(feature, nn.ReLU):\n",
        "        modified_layers[\"dropout\"] = nn.Dropout(p=0.3)\n",
        "\n",
        "    model_train_reg.layer1[indx] = nn.Sequential(modified_layers)\n",
        "\n",
        "\n",
        "\n",
        "  layer2 = model_train_reg.layer2\n",
        "\n",
        "  for indx in [1]:\n",
        "    modified_layers = OrderedDict()\n",
        "    for name, feature in layer2[indx].named_children():\n",
        "      modified_layers[name] = feature\n",
        "      if isinstance(feature, nn.ReLU):\n",
        "        modified_layers[\"dropout\"] = nn.Dropout(p=0.3)\n",
        "\n",
        "    model_train_reg.layer2[indx] = nn.Sequential(modified_layers)\n",
        "\n",
        "  layer3 = model_train_reg.layer3\n",
        "\n",
        "  for indx in [1]:\n",
        "    modified_layers = OrderedDict()\n",
        "    for name, feature in layer3[indx].named_children():\n",
        "      modified_layers[name] = feature\n",
        "      if isinstance(feature, nn.ReLU):\n",
        "        modified_layers[\"dropout\"] = nn.Dropout(p=0.3)\n",
        "    model_train_reg.layer3[indx] = nn.Sequential(modified_layers)\n",
        "\n",
        "  layer4 = model_train_reg.layer4\n",
        "\n",
        "  for indx in [1]:\n",
        "    modified_layers = OrderedDict()\n",
        "    for name, feature in layer4[indx].named_children():\n",
        "      modified_layers[name] = feature\n",
        "      if isinstance(feature, nn.ReLU):\n",
        "        modified_layers[\"dropout\"] = nn.Dropout(p=0.3)\n",
        "\n",
        "    model_train_reg.layer4[indx] = nn.Sequential(modified_layers)\n",
        "\n",
        "  return model_train_reg"
      ],
      "metadata": {
        "id": "2fQbrANUGdQZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"----- 5 PERCENT DROPOUT -------\")\n",
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "# Model parameters\n",
        "model_name = \"Resnet18 Dropout\"\n",
        "model_params = \"cifar10_resnet18_l2_dropout_regularization.pt\"\n",
        "\n",
        "# Percantage of class to forget\n",
        "amount = 0.05\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "# mia scores\n",
        "MIA_scores_before = []\n",
        "MIA_scores = []\n",
        "\n",
        "# Accuracy\n",
        "Accuracy_forget_before = []\n",
        "\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "\n",
        "\n",
        "print(f\"Model name: {model_name}\")\n",
        "print(f\"Amount: {100 * amount}\")\n",
        "\n",
        "for class_num in classes:\n",
        "  print(f\"------ UNLEARNING CLASS {class_num} ------\\n\")\n",
        "  # Define model from trained params\n",
        "\n",
        "  model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "  model_forget_ft = model_forget_ft.to(DEVICE)\n",
        "  model_forget_ft = modify_model(model_forget_ft)\n",
        "\n",
        "  model_forget_ft.load_state_dict(torch.load(model_params))\n",
        "  model_forget_ft.eval()\n",
        "\n",
        "  # --- FORMING DATASET ------\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == class_num)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # ---------\n",
        "\n",
        "  # --- EVALUATING MODEL BEFORE ----\n",
        "  accuracy_forget = 100.0 * accuracy(model_forget_ft, forget_loader)\n",
        "  Accuracy_forget_before.append(accuracy_forget)\n",
        "  print(f\"Accuracy before on forget set: {accuracy_forget}\")\n",
        "\n",
        "  mia_attack = train_mia_on_class(model_forget_ft, class_num, forget_set, test_set, True, 2)\n",
        "  MIA_scores_before.append(mia_attack)\n",
        "  print(f\"MIA score before on forget set: {np.round(mia_attack, 3)}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  model_forget_ft = model_forget_ft.to(DEVICE)\n",
        "  # --------------\n",
        "\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning(model_forget_ft, retain_loader, forget_loader, test_loader)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "  print()\n",
        "  model_ft_forget.eval()\n",
        "  mia_scores_fr = train_mia_on_class(model_forget_ft, class_num, forget_set, test_set, True, 5)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr:.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "  print()\n",
        "  print('-' * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQEsehIrKXTj",
        "outputId": "05777a09-4be8-4d89-c07a-3ee88dde85af"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- 5 PERCENT DROPOUT -------\n",
            "Model name: Resnet18 Dropout\n",
            "Amount: 5.0\n",
            "------ UNLEARNING CLASS 0 ------\n",
            "\n",
            "Accuracy before on forget set: 84.0\n",
            "MIA score before on forget set: 0.93\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 60.9%\n",
            "Test set accuracy FORGET model: 59.2%\n",
            "Forget set accuracy FORGET model: 60.8%\n",
            "\n",
            "The MIA has an accuracy of 0.888 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 1 ------\n",
            "\n",
            "Accuracy before on forget set: 87.6\n",
            "MIA score before on forget set: 0.89\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 67.3%\n",
            "Test set accuracy FORGET model: 65.9%\n",
            "Forget set accuracy FORGET model: 80.8%\n",
            "\n",
            "The MIA has an accuracy of 0.892 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 2 ------\n",
            "\n",
            "Accuracy before on forget set: 69.6\n",
            "MIA score before on forget set: 0.81\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 62.5%\n",
            "Test set accuracy FORGET model: 60.9%\n",
            "Forget set accuracy FORGET model: 33.6%\n",
            "\n",
            "The MIA has an accuracy of 0.844 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 3 ------\n",
            "\n",
            "Accuracy before on forget set: 60.0\n",
            "MIA score before on forget set: 0.89\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 63.6%\n",
            "Test set accuracy FORGET model: 62.0%\n",
            "Forget set accuracy FORGET model: 36.8%\n",
            "\n",
            "The MIA has an accuracy of 0.824 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 4 ------\n",
            "\n",
            "Accuracy before on forget set: 76.4\n",
            "MIA score before on forget set: 0.9\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 61.9%\n",
            "Test set accuracy FORGET model: 61.5%\n",
            "Forget set accuracy FORGET model: 77.2%\n",
            "\n",
            "The MIA has an accuracy of 0.856 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 5 ------\n",
            "\n",
            "Accuracy before on forget set: 70.39999999999999\n",
            "MIA score before on forget set: 0.81\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 65.7%\n",
            "Test set accuracy FORGET model: 64.3%\n",
            "Forget set accuracy FORGET model: 45.2%\n",
            "\n",
            "The MIA has an accuracy of 0.820 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 6 ------\n",
            "\n",
            "Accuracy before on forget set: 85.2\n",
            "MIA score before on forget set: 0.87\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 60.7%\n",
            "Test set accuracy FORGET model: 59.7%\n",
            "Forget set accuracy FORGET model: 84.4%\n",
            "\n",
            "The MIA has an accuracy of 0.872 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 7 ------\n",
            "\n",
            "Accuracy before on forget set: 84.39999999999999\n",
            "MIA score before on forget set: 0.91\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 64.9%\n",
            "Test set accuracy FORGET model: 63.1%\n",
            "Forget set accuracy FORGET model: 60.4%\n",
            "\n",
            "The MIA has an accuracy of 0.904 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 8 ------\n",
            "\n",
            "Accuracy before on forget set: 87.6\n",
            "MIA score before on forget set: 0.9\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 56.1%\n",
            "Test set accuracy FORGET model: 55.2%\n",
            "Forget set accuracy FORGET model: 74.8%\n",
            "\n",
            "The MIA has an accuracy of 0.792 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 9 ------\n",
            "\n",
            "Accuracy before on forget set: 87.2\n",
            "MIA score before on forget set: 0.85\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 57.1%\n",
            "Test set accuracy FORGET model: 55.8%\n",
            "Forget set accuracy FORGET model: 59.6%\n",
            "\n",
            "The MIA has an accuracy of 0.872 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# mia scores\n",
        "print(MIA_scores_before)\n",
        "print(MIA_scores)\n",
        "\n",
        "# Accuracy\n",
        "print(Accuracy_forget_before)\n",
        "\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_V6kabXKYRX",
        "outputId": "b9c64726-a1a8-41e0-f6e4-d7153262bf07"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9299999999999999, 0.89, 0.81, 0.89, 0.9, 0.81, 0.87, 0.9099999999999999, 0.9, 0.85]\n",
            "[0.8879999999999999, 0.892, 0.844, 0.8240000000000001, 0.8559999999999999, 0.82, 0.8720000000000001, 0.9040000000000001, 0.792, 0.8719999999999999]\n",
            "[84.0, 87.6, 69.6, 60.0, 76.4, 70.39999999999999, 85.2, 84.39999999999999, 87.6, 87.2]\n",
            "[60.94060898402171, 67.32585714859923, 62.47814290021103, 63.613891233570484, 61.90208216094541, 65.65853462481411, 60.748095592225596, 64.93488222525926, 56.10667845730249, 57.11184805547182]\n",
            "[60.8, 80.80000000000001, 33.6, 36.8, 77.2, 45.2, 84.39999999999999, 60.4, 74.8, 59.599999999999994]\n",
            "[59.209999999999994, 65.94, 60.919999999999995, 61.980000000000004, 61.47, 64.28, 59.730000000000004, 63.13999999999999, 55.22, 55.769999999999996]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "mean_mia_score_before_dropout = np.mean(np.array(MIA_scores_before))\n",
        "mean_mia_score_after_dropout = np.mean(np.array(MIA_scores))\n",
        "\n",
        "mean_accuracy_forget_before_dropout = np.mean(np.array(Accuracy_forget_before))\n",
        "\n",
        "mean_accuracy_forget_after_dropout = np.mean(np.array(Accuracy_forget))\n",
        "mean_accuracy_retain_dropout = np.mean(np.array(Accuracy_retain))\n",
        "mean_accuracy_test_dropout = np.mean(np.array(Accuracy_test))"
      ],
      "metadata": {
        "id": "j7Yh6CADKZip"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(mean_mia_score_before_dropout)\n",
        "print(mean_mia_score_after_dropout)\n",
        "\n",
        "print(mean_accuracy_forget_before_dropout)\n",
        "\n",
        "print(mean_accuracy_forget_after_dropout)\n",
        "print(mean_accuracy_retain_dropout)\n",
        "print(mean_accuracy_test_dropout)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zragInsdKada",
        "outputId": "645ad06d-cc2b-4d3b-850e-541ec1661d62"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.876\n",
            "0.8564\n",
            "79.24\n",
            "61.36\n",
            "62.08206213824211\n",
            "60.766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentation model"
      ],
      "metadata": {
        "id": "d4z80pJ_Kcwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CIFAR 10 dataset\n",
        "torch.manual_seed(1991)\n",
        "# Transformations\n",
        "normalize = transforms.Compose(\n",
        "    [\n",
        "         transforms.RandomRotation(30), # Randomly rotate some images by 20 degrees\n",
        "         transforms.RandomHorizontalFlip(), # Randomly horizontal flip the images\n",
        "         transforms.ColorJitter(brightness = 0.1, # Randomly adjust color jitter of the images\n",
        "                                                            contrast = 0.1,\n",
        "                                                            saturation = 0.1),\n",
        "         transforms.RandomAdjustSharpness(sharpness_factor = 2,\n",
        "                                                                      p = 0.1), # Randomly adjust sharpness\n",
        "          transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "to_tensor_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Train data\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=normalize\n",
        ")\n",
        "# Train loader\n",
        "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "# Test data\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=to_tensor_transform\n",
        ")\n",
        "# Test loader\n",
        "test_loader = DataLoader(test_set, batch_size=256, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knP7-AlHKbaQ",
        "outputId": "599a3c0b-07d4-44cf-d81f-2e19a9de9a25"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"----- 5 PERCENT -------\")\n",
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "# Model parameters\n",
        "model_name = \"Resnet18 Augmentation\"\n",
        "model_params = \"cifar10_resnet18_augmentation.pt\"\n",
        "\n",
        "# Percantage of class to forget\n",
        "amount = 0.05\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "# mia scores\n",
        "MIA_scores_before = []\n",
        "MIA_scores = []\n",
        "\n",
        "# Accuracy\n",
        "Accuracy_forget_before = []\n",
        "\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "\n",
        "\n",
        "print(f\"Model name: {model_name}\")\n",
        "print(f\"Amount: {100 * amount}\")\n",
        "\n",
        "for class_num in classes:\n",
        "  print(f\"------ UNLEARNING CLASS {class_num} ------\\n\")\n",
        "  # Define model from trained params\n",
        "\n",
        "  model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "  model_forget_ft.load_state_dict(torch.load(model_params))\n",
        "  model_forget_ft.eval()\n",
        "  model_forget_ft = model_forget_ft.to(DEVICE)\n",
        "\n",
        "  # --- FORMING DATASET ------\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == class_num)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # ---------\n",
        "\n",
        "  # --- EVALUATING MODEL BEFORE ----\n",
        "  accuracy_forget = 100.0 * accuracy(model_forget_ft, forget_loader)\n",
        "  Accuracy_forget_before.append(accuracy_forget)\n",
        "  print(f\"Accuracy before on forget set: {accuracy_forget}\")\n",
        "\n",
        "  mia_attack = train_mia_on_class(model_forget_ft, class_num, forget_set, test_set, True, 2)\n",
        "  MIA_scores_before.append(mia_attack)\n",
        "  print(f\"MIA score before on forget set: {np.round(mia_attack, 3)}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  model_forget_ft = model_forget_ft.to(DEVICE)\n",
        "  # --------------\n",
        "\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning(model_forget_ft, retain_loader, forget_loader, test_loader)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "  print()\n",
        "  model_forget_ft.eval()\n",
        "  mia_scores_fr = train_mia_on_class(model_forget_ft, class_num, forget_set, test_set, True, 5)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr:.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "  print()\n",
        "  print('-' * 20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y_BCYYXKd_4",
        "outputId": "3192df84-6e5e-4d03-b1d7-d73a04dd685d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- 5 PERCENT -------\n",
            "Model name: Resnet18 Augmentation\n",
            "Amount: 5.0\n",
            "------ UNLEARNING CLASS 0 ------\n",
            "\n",
            "Accuracy before on forget set: 81.2\n",
            "MIA score before on forget set: 0.87\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 51.4%\n",
            "Test set accuracy FORGET model: 55.4%\n",
            "Forget set accuracy FORGET model: 46.4%\n",
            "\n",
            "The MIA has an accuracy of 0.808 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 1 ------\n",
            "\n",
            "Accuracy before on forget set: 78.8\n",
            "MIA score before on forget set: 0.92\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 52.4%\n",
            "Test set accuracy FORGET model: 55.8%\n",
            "Forget set accuracy FORGET model: 64.8%\n",
            "\n",
            "The MIA has an accuracy of 0.844 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 2 ------\n",
            "\n",
            "Accuracy before on forget set: 73.6\n",
            "MIA score before on forget set: 0.82\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 50.9%\n",
            "Test set accuracy FORGET model: 54.4%\n",
            "Forget set accuracy FORGET model: 40.8%\n",
            "\n",
            "The MIA has an accuracy of 0.756 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 3 ------\n",
            "\n",
            "Accuracy before on forget set: 65.2\n",
            "MIA score before on forget set: 0.81\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 51.4%\n",
            "Test set accuracy FORGET model: 54.8%\n",
            "Forget set accuracy FORGET model: 21.2%\n",
            "\n",
            "The MIA has an accuracy of 0.788 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 4 ------\n",
            "\n",
            "Accuracy before on forget set: 76.8\n",
            "MIA score before on forget set: 0.91\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 51.6%\n",
            "Test set accuracy FORGET model: 54.5%\n",
            "Forget set accuracy FORGET model: 43.2%\n",
            "\n",
            "The MIA has an accuracy of 0.800 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 5 ------\n",
            "\n",
            "Accuracy before on forget set: 68.8\n",
            "MIA score before on forget set: 0.84\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 51.5%\n",
            "Test set accuracy FORGET model: 55.3%\n",
            "Forget set accuracy FORGET model: 30.4%\n",
            "\n",
            "The MIA has an accuracy of 0.852 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 6 ------\n",
            "\n",
            "Accuracy before on forget set: 82.0\n",
            "MIA score before on forget set: 0.83\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 49.6%\n",
            "Test set accuracy FORGET model: 52.8%\n",
            "Forget set accuracy FORGET model: 60.4%\n",
            "\n",
            "The MIA has an accuracy of 0.864 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 7 ------\n",
            "\n",
            "Accuracy before on forget set: 81.2\n",
            "MIA score before on forget set: 0.91\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 52.8%\n",
            "Test set accuracy FORGET model: 56.3%\n",
            "Forget set accuracy FORGET model: 60.8%\n",
            "\n",
            "The MIA has an accuracy of 0.824 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 8 ------\n",
            "\n",
            "Accuracy before on forget set: 89.2\n",
            "MIA score before on forget set: 0.82\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 51.4%\n",
            "Test set accuracy FORGET model: 55.1%\n",
            "Forget set accuracy FORGET model: 69.6%\n",
            "\n",
            "The MIA has an accuracy of 0.784 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n",
            "------ UNLEARNING CLASS 9 ------\n",
            "\n",
            "Accuracy before on forget set: 77.60000000000001\n",
            "MIA score before on forget set: 0.84\n",
            "\n",
            "\n",
            "Retain set accuracy FORGET model: 53.4%\n",
            "Test set accuracy FORGET model: 57.0%\n",
            "Forget set accuracy FORGET model: 45.2%\n",
            "\n",
            "The MIA has an accuracy of 0.808 on forgotten vs unseen images on FORGET model\n",
            "\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# mia scores\n",
        "print(MIA_scores_before)\n",
        "print(MIA_scores)\n",
        "\n",
        "# Accuracy\n",
        "print(Accuracy_forget_before)\n",
        "\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhC9vVcNKe49",
        "outputId": "4d8badb2-d525-4cb2-bf51-589ea8dcab86"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.87, 0.9199999999999999, 0.82, 0.81, 0.91, 0.84, 0.8300000000000001, 0.91, 0.8200000000000001, 0.84]\n",
            "[0.808, 0.844, 0.756, 0.788, 0.8, 0.852, 0.8640000000000001, 0.8240000000000001, 0.784, 0.808]\n",
            "[81.2, 78.8, 73.6, 65.2, 76.8, 68.8, 82.0, 81.2, 89.2, 77.60000000000001]\n",
            "[51.30648014150185, 52.39674404582454, 51.068062616803644, 51.25097968288419, 51.75955141990072, 51.28828683977812, 49.40809968847352, 52.68795594766777, 51.545480123799194, 52.78654259702152]\n",
            "[50.0, 62.8, 38.800000000000004, 28.000000000000004, 48.8, 30.0, 60.0, 62.8, 66.4, 45.6]\n",
            "[55.400000000000006, 55.800000000000004, 54.44, 54.82, 54.510000000000005, 55.26, 52.75, 56.26, 55.11000000000001, 57.05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "mean_mia_score_before_aug = np.mean(np.array(MIA_scores_before))\n",
        "mean_mia_score_after_aug = np.mean(np.array(MIA_scores))\n",
        "\n",
        "mean_accuracy_forget_before_aug = np.mean(np.array(Accuracy_forget_before))\n",
        "\n",
        "mean_accuracy_forget_after_aug = np.mean(np.array(Accuracy_forget))\n",
        "mean_accuracy_retain_aug = np.mean(np.array(Accuracy_retain))\n",
        "mean_accuracy_test_aug = np.mean(np.array(Accuracy_test))"
      ],
      "metadata": {
        "id": "LPQyoTEbKgG0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(mean_mia_score_before_aug)\n",
        "print(mean_mia_score_after_aug)\n",
        "\n",
        "print(mean_accuracy_forget_before_aug)\n",
        "\n",
        "print(mean_accuracy_forget_after_aug)\n",
        "print(mean_accuracy_retain_aug)\n",
        "print(mean_accuracy_test_aug)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53QHP7XLKgyT",
        "outputId": "91e2b278-0a24-4830-e436-6bd5b84b4079"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.857\n",
            "0.8128\n",
            "77.44\n",
            "49.32000000000001\n",
            "51.5498183103655\n",
            "55.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hcq8JzK5KiFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}