{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75173d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, dataset, Subset\n",
    "class ParameterPerturber:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        opt,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        parameters=None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.opt = opt\n",
    "        self.device = device\n",
    "        self.alpha = None\n",
    "        self.xmin = None\n",
    "\n",
    "        # print(parameters)\n",
    "        self.lower_bound = parameters[\"lower_bound\"]\n",
    "        self.exponent = parameters[\"exponent\"]\n",
    "        self.magnitude_diff = parameters[\"magnitude_diff\"]  # unused\n",
    "        self.min_layer = parameters[\"min_layer\"]\n",
    "        self.max_layer = parameters[\"max_layer\"]\n",
    "        self.forget_threshold = parameters[\"forget_threshold\"]\n",
    "        self.dampening_constant = parameters[\"dampening_constant\"]\n",
    "        self.selection_weighting = parameters[\"selection_weighting\"]\n",
    "\n",
    "    def get_layer_num(self, layer_name: str) -> int:\n",
    "        layer_id = layer_name.split(\".\")[1]\n",
    "        if layer_id.isnumeric():\n",
    "            return int(layer_id)\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def zerolike_params_dict(self, model: torch.nn) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Taken from: Avalanche: an End-to-End Library for Continual Learning - https://github.com/ContinualAI/avalanche\n",
    "        Returns a dict like named_parameters(), with zeroed-out parameter valuse\n",
    "        Parameters:\n",
    "        model (torch.nn): model to get param dict from\n",
    "        Returns:\n",
    "        dict(str,torch.Tensor): dict of zero-like params\n",
    "        \"\"\"\n",
    "        return dict(\n",
    "            [\n",
    "                (k, torch.zeros_like(p, device=p.device))\n",
    "                for k, p in model.named_parameters()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def fulllike_params_dict(\n",
    "        self, model: torch.nn, fill_value, as_tensor: bool = False\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns a dict like named_parameters(), with parameter values replaced with fill_value\n",
    "\n",
    "        Parameters:\n",
    "        model (torch.nn): model to get param dict from\n",
    "        fill_value: value to fill dict with\n",
    "        Returns:\n",
    "        dict(str,torch.Tensor): dict of named_parameters() with filled in values\n",
    "        \"\"\"\n",
    "\n",
    "        def full_like_tensor(fillval, shape: list) -> list:\n",
    "            \"\"\"\n",
    "            recursively builds nd list of shape shape, filled with fillval\n",
    "            Parameters:\n",
    "            fillval: value to fill matrix with\n",
    "            shape: shape of target tensor\n",
    "            Returns:\n",
    "            list of shape shape, filled with fillval at each index\n",
    "            \"\"\"\n",
    "            if len(shape) > 1:\n",
    "                fillval = full_like_tensor(fillval, shape[1:])\n",
    "            tmp = [fillval for _ in range(shape[0])]\n",
    "            return tmp\n",
    "\n",
    "        dictionary = {}\n",
    "\n",
    "        for n, p in model.named_parameters():\n",
    "            _p = (\n",
    "                torch.tensor(full_like_tensor(fill_value, p.shape), device=self.device)\n",
    "                if as_tensor\n",
    "                else full_like_tensor(fill_value, p.shape)\n",
    "            )\n",
    "            dictionary[n] = _p\n",
    "        return dictionary\n",
    "\n",
    "    def subsample_dataset(self, dataset: dataset, sample_perc: float) -> Subset:\n",
    "        \"\"\"\n",
    "        Take a subset of the dataset\n",
    "\n",
    "        Parameters:\n",
    "        dataset (dataset): dataset to be subsampled\n",
    "        sample_perc (float): percentage of dataset to sample. range(0,1)\n",
    "        Returns:\n",
    "        Subset (float): requested subset of the dataset\n",
    "        \"\"\"\n",
    "        sample_idxs = np.arange(0, len(dataset), step=int((1 / sample_perc)))\n",
    "        return Subset(dataset, sample_idxs)\n",
    "\n",
    "    def split_dataset_by_class(self, dataset: dataset) -> List[Subset]:\n",
    "        \"\"\"\n",
    "        Split dataset into list of subsets\n",
    "            each idx corresponds to samples from that class\n",
    "\n",
    "        Parameters:\n",
    "        dataset (dataset): dataset to be split\n",
    "        Returns:\n",
    "        subsets (List[Subset]): list of subsets of the dataset,\n",
    "            each containing only the samples belonging to that class\n",
    "        \"\"\"\n",
    "        n_classes = len(set([target for _, target in dataset]))\n",
    "        subset_idxs = [[] for _ in range(n_classes)]\n",
    "        for idx, (x, y) in enumerate(dataset):\n",
    "            subset_idxs[y].append(idx)\n",
    "\n",
    "        return [Subset(dataset, subset_idxs[idx]) for idx in range(n_classes)]\n",
    "\n",
    "    def calc_importance(self, dataloader: DataLoader) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Adapated from: Avalanche: an End-to-End Library for Continual Learning - https://github.com/ContinualAI/avalanche\n",
    "        Calculate per-parameter, importance\n",
    "            returns a dictionary [param_name: list(importance per parameter)]\n",
    "        Parameters:\n",
    "        DataLoader (DataLoader): DataLoader to be iterated over\n",
    "        Returns:\n",
    "        importances (dict(str, torch.Tensor([]))): named_parameters-like dictionary containing list of importances for each parameter\n",
    "        \"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        importances = self.zerolike_params_dict(self.model)\n",
    "        for sample in dataloader:\n",
    "            x, y = sample[0], sample[1]\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            self.opt.zero_grad()\n",
    "            out = self.model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "\n",
    "            for (k1, p), (k2, imp) in zip(\n",
    "                self.model.named_parameters(), importances.items()\n",
    "            ):\n",
    "                if p.grad is not None:\n",
    "                    imp.data += p.grad.data.clone().pow(2)\n",
    "\n",
    "        # average over mini batch length\n",
    "        for _, imp in importances.items():\n",
    "            imp.data /= float(len(dataloader))\n",
    "        return importances\n",
    "\n",
    "    def modify_weight(\n",
    "        self,\n",
    "        original_importance: List[Dict[str, torch.Tensor]],\n",
    "        forget_importance: List[Dict[str, torch.Tensor]],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perturb weights based on the SSD equations given in the paper\n",
    "        Parameters:\n",
    "        original_importance (List[Dict[str, torch.Tensor]]): list of importances for original dataset\n",
    "        forget_importance (List[Dict[str, torch.Tensor]]): list of importances for forget sample\n",
    "        threshold (float): value to multiply original imp by to determine memorization.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (n, p), (oimp_n, oimp), (fimp_n, fimp) in zip(\n",
    "                self.model.named_parameters(),\n",
    "                original_importance.items(),\n",
    "                forget_importance.items(),\n",
    "            ):\n",
    "                # Synapse Selection with parameter alpha\n",
    "                oimp_norm = oimp.mul(self.selection_weighting)\n",
    "                locations = torch.where(fimp > oimp_norm)\n",
    "\n",
    "                # Synapse Dampening with parameter lambda\n",
    "                weight = ((oimp.mul(self.dampening_constant)).div(fimp)).pow(\n",
    "                    self.exponent\n",
    "                )\n",
    "                update = weight[locations]\n",
    "                # Bound by 1 to prevent parameter values to increase.\n",
    "                min_locs = torch.where(update > self.lower_bound)\n",
    "                update[min_locs] = self.lower_bound\n",
    "                p[locations] = p[locations].mul(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlearning_pertubation(\n",
    "    net, \n",
    "    retain_loader, \n",
    "    forget_loader, \n",
    "    val_loader):\n",
    "    \n",
    "    epochs = 1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001,\n",
    "                      momentum=0.9, weight_decay=0)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=epochs)\n",
    "\n",
    "    alpha = 1 # alpha in the paper\n",
    "    lambda_ = 10 # lambda in the paper\n",
    "    selection_weighting = 10 * alpha\n",
    "\n",
    "    parameters = {\n",
    "        \"lower_bound\": 1,\n",
    "        \"exponent\": 1,\n",
    "        \"magnitude_diff\": None,\n",
    "        \"min_layer\": -1,\n",
    "        \"max_layer\": -1,\n",
    "        \"forget_threshold\": 1,\n",
    "        \"dampening_constant\": lambda_,\n",
    "        \"selection_weighting\": selection_weighting,\n",
    "    }\n",
    "    \n",
    "    full_train_dl = DataLoader(\n",
    "    ConcatDataset((retain_loader.dataset, forget_loader.dataset)),\n",
    "    batch_size=64,\n",
    "    )\n",
    "\n",
    "    pdr = ParameterPerturber(net, optimizer, DEVICE, parameters)\n",
    "\n",
    "    net = net.eval()\n",
    "\n",
    "    sample_importances = pdr.calc_importance(forget_loader)\n",
    "    original_importances = pdr.calc_importance(full_train_dl)\n",
    "    pdr.modify_weight(original_importances, sample_importances)\n",
    "\n",
    "    net.eval()\n",
    "    return net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
