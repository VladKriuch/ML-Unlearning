{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys-grbQTQApH",
        "outputId": "125e4050-b88f-4d32-8125-4d07bd4cba2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: CUDA\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model, model_selection\n",
        "\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models import resnet18\n",
        "import numpy as np\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on device:\", DEVICE.upper())\n",
        "\n",
        "# manual random seed is used for dataset partitioning\n",
        "# to ensure reproducible results across runs\n",
        "RNG = torch.Generator().manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory"
      ],
      "metadata": {
        "id": "p1r58T5xQUpG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR 10 dataset\n",
        "normalize = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train data\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=normalize\n",
        ")\n",
        "# Train loader\n",
        "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "# Test data\n",
        "# we split held out data into test and validation set\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=normalize\n",
        ")\n",
        "test_loader = DataLoader(test_set, batch_size=256, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz_mQmXOQWqu",
        "outputId": "1c7835c6-c648-4417-a4de-802011e3d38f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13188017.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.where(np.array(train_set.targets) == 1)[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5G3h37GQZS0",
        "outputId": "d209931f-cfb9-4800-a269-14d209c643ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose random forget indecies from cars\n",
        "# Index of class\n",
        "class_index = 1\n",
        "class_set = np.where(np.array(train_set.targets) == 1)[0]\n",
        "# Percantage of whole data ( from class )\n",
        "amount = 0.1 # 10 %\n",
        "amount_int = class_set.shape[0] * amount\n",
        "\n",
        "# Get indeces\n",
        "forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "# construct indices of retain from those of the forget set\n",
        "forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "forget_mask[forget_idx] = True\n",
        "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "# split train set into a forget and a retain set\n",
        "forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "forget_loader = torch.utils.data.DataLoader(\n",
        "    forget_set, batch_size=256, shuffle=True, num_workers=2\n",
        ")\n",
        "retain_loader = torch.utils.data.DataLoader(\n",
        "    retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        ")\n"
      ],
      "metadata": {
        "id": "VJOStqaJQcU2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\": test_loader\n",
        "}\n",
        "\n",
        "dataset_sizes = {\"train\": len(train_set), \"val\": len(test_set)}\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    with TemporaryDirectory() as tempdir:\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs = inputs.to(DEVICE)\n",
        "                    labels = labels.to(DEVICE)\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                if phase == 'train':\n",
        "                    scheduler.step()\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "        # load best model weights\n",
        "        model.load_state_dict(torch.load(best_model_params_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "ttNfvarJQeFi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = resnet18(weights=None, num_classes=10)\n",
        "# num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 10\n",
        "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
        "# model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "5e18OhSvQh1e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFqYSspcQkEz",
        "outputId": "c52f9f70-d3e0-48fe-b8d7-7d443930bbca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/14\n",
            "----------\n",
            "train Loss: 1.3797 Acc: 0.5043\n",
            "val Loss: 1.1638 Acc: 0.5866\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 0.9742 Acc: 0.6572\n",
            "val Loss: 0.9782 Acc: 0.6557\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.7955 Acc: 0.7217\n",
            "val Loss: 0.9502 Acc: 0.6741\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.6601 Acc: 0.7668\n",
            "val Loss: 0.8870 Acc: 0.6997\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.5535 Acc: 0.8049\n",
            "val Loss: 0.9101 Acc: 0.7031\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.4575 Acc: 0.8372\n",
            "val Loss: 0.8126 Acc: 0.7381\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.3671 Acc: 0.8693\n",
            "val Loss: 0.8807 Acc: 0.7344\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.1551 Acc: 0.9525\n",
            "val Loss: 0.7870 Acc: 0.7758\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.0804 Acc: 0.9780\n",
            "val Loss: 0.8682 Acc: 0.7738\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.0464 Acc: 0.9896\n",
            "val Loss: 0.9669 Acc: 0.7718\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.0252 Acc: 0.9953\n",
            "val Loss: 1.0843 Acc: 0.7668\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.0133 Acc: 0.9980\n",
            "val Loss: 1.2156 Acc: 0.7684\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.0076 Acc: 0.9990\n",
            "val Loss: 1.3208 Acc: 0.7669\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.0045 Acc: 0.9996\n",
            "val Loss: 1.3988 Acc: 0.7686\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.0027 Acc: 0.9999\n",
            "val Loss: 1.4177 Acc: 0.7709\n",
            "\n",
            "Training complete in 5m 14s\n",
            "Best val Acc: 0.775800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h28_D54zQly7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.prune as prune\n",
        "from math import sqrt\n",
        "import json\n",
        "from copy import deepcopy\n",
        "\n",
        "def kl_loss_fn(outputs, dist_target):\n",
        "    kl_loss = F.kl_div(torch.log_softmax(outputs, dim=1), dist_target, log_target=True, reduction='batchmean')\n",
        "    return kl_loss\n",
        "\n",
        "def entropy_loss_fn(outputs, labels, dist_target, class_weights):\n",
        "    ce_loss = F.cross_entropy(outputs, labels, weight=class_weights)\n",
        "    entropy_dist_target = torch.sum(-torch.exp(dist_target) * dist_target, dim=1)\n",
        "    entropy_outputs = torch.sum(-torch.softmax(outputs, dim=1) * torch.log_softmax(outputs, dim=1), dim=1)\n",
        "    entropy_loss = F.mse_loss(entropy_outputs, entropy_dist_target)\n",
        "    return ce_loss + entropy_loss\n",
        "\n",
        "\n",
        "def unlearning(\n",
        "    net,\n",
        "    retain_loader,\n",
        "    forget_loader,\n",
        "    val_loader,\n",
        "    class_weights=None,\n",
        "):\n",
        "    \"\"\"Simple unlearning by finetuning.\"\"\"\n",
        "    epochs = 3.2\n",
        "    max_iters = int(len(retain_loader) * epochs)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.0005,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    initial_net = deepcopy(net)\n",
        "\n",
        "    net.train()\n",
        "    initial_net.eval()\n",
        "\n",
        "    def prune_model(net, amount=0.95, rand_init=True):\n",
        "        # Modules to prune\n",
        "        modules = list()\n",
        "        for k, m in enumerate(net.modules()):\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                modules.append((m, 'weight'))\n",
        "                if m.bias is not None:\n",
        "                    modules.append((m, 'bias'))\n",
        "\n",
        "        # Prune criteria\n",
        "        prune.global_unstructured(\n",
        "            modules,\n",
        "            #pruning_method=prune.RandomUnstructured,\n",
        "            pruning_method=prune.L1Unstructured,\n",
        "            amount=amount,\n",
        "        )\n",
        "\n",
        "        # Perform the prune\n",
        "        for k, m in enumerate(net.modules()):\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                prune.remove(m, 'weight')\n",
        "                if m.bias is not None:\n",
        "                    prune.remove(m, 'bias')\n",
        "\n",
        "        # Random initialization\n",
        "        if rand_init:\n",
        "            for k, m in enumerate(net.modules()):\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    mask = m.weight == 0\n",
        "                    c_in = mask.shape[1]\n",
        "                    k = 1/(c_in*mask.shape[2]*mask.shape[3])\n",
        "                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n",
        "                    m.weight.data[mask] = randinit[mask]\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    mask = m.weight == 0\n",
        "                    c_in = mask.shape[1]\n",
        "                    k = 1/c_in\n",
        "                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n",
        "                    m.weight.data[mask] = randinit[mask]\n",
        "\n",
        "    num_iters = 0\n",
        "    running = True\n",
        "    prune_amount = 0.99\n",
        "    prune_model(net, prune_amount, True)\n",
        "    while running:\n",
        "        net.train()\n",
        "        for sample in retain_loader:\n",
        "            inputs = sample[0]\n",
        "            targets = sample[1]\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "            # Get target distribution\n",
        "            with torch.no_grad():\n",
        "                original_outputs = initial_net(inputs)\n",
        "                preds = torch.log_softmax(original_outputs, dim=1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = entropy_loss_fn(outputs, targets, preds, class_weights)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            num_iters += 1\n",
        "            # Stop at max iters\n",
        "            if num_iters > max_iters:\n",
        "                running = False\n",
        "                break\n",
        "\n",
        "    net.eval()"
      ],
      "metadata": {
        "id": "zX2K5STjWJZr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model_ft_forget = unlearning(model_ft, retain_loader, forget_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omVPIY18TZri",
        "outputId": "d98b5b5a-593b-4993-c603-9a1f722aed04"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 45.7 s, sys: 1.3 s, total: 47 s\n",
            "Wall time: 1min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(forget_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHwTHdd8ThqF",
        "outputId": "f93b77e6-4c41-4992-a9d7-fb9866018bb1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smyceZBuTrGr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}