{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys-grbQTQApH",
        "outputId": "125e4050-b88f-4d32-8125-4d07bd4cba2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: CUDA\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model, model_selection\n",
        "\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models import resnet18\n",
        "import numpy as np\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on device:\", DEVICE.upper())\n",
        "\n",
        "# manual random seed is used for dataset partitioning\n",
        "# to ensure reproducible results across runs\n",
        "RNG = torch.Generator().manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory"
      ],
      "metadata": {
        "id": "p1r58T5xQUpG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR 10 dataset\n",
        "normalize = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train data\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=normalize\n",
        ")\n",
        "# Train loader\n",
        "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "# Test data\n",
        "# we split held out data into test and validation set\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=normalize\n",
        ")\n",
        "test_loader = DataLoader(test_set, batch_size=256, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz_mQmXOQWqu",
        "outputId": "1c7835c6-c648-4417-a4de-802011e3d38f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13188017.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.where(np.array(train_set.targets) == 1)[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5G3h37GQZS0",
        "outputId": "d209931f-cfb9-4800-a269-14d209c643ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose random forget indecies from cars\n",
        "# Index of class\n",
        "class_index = 1\n",
        "class_set = np.where(np.array(train_set.targets) == 1)[0]\n",
        "# Percantage of whole data ( from class )\n",
        "amount = 0.1 # 10 %\n",
        "amount_int = class_set.shape[0] * amount\n",
        "\n",
        "# Get indeces\n",
        "forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "# construct indices of retain from those of the forget set\n",
        "forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "forget_mask[forget_idx] = True\n",
        "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "# split train set into a forget and a retain set\n",
        "forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "forget_loader = torch.utils.data.DataLoader(\n",
        "    forget_set, batch_size=256, shuffle=True, num_workers=2\n",
        ")\n",
        "retain_loader = torch.utils.data.DataLoader(\n",
        "    retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        ")\n"
      ],
      "metadata": {
        "id": "VJOStqaJQcU2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\": test_loader\n",
        "}\n",
        "\n",
        "dataset_sizes = {\"train\": len(train_set), \"val\": len(test_set)}\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    with TemporaryDirectory() as tempdir:\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs = inputs.to(DEVICE)\n",
        "                    labels = labels.to(DEVICE)\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                if phase == 'train':\n",
        "                    scheduler.step()\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "        # load best model weights\n",
        "        model.load_state_dict(torch.load(best_model_params_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "ttNfvarJQeFi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = resnet18(weights=None, num_classes=10)\n",
        "# num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 10\n",
        "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
        "# model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "5e18OhSvQh1e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFqYSspcQkEz",
        "outputId": "96f19d88-1709-4dd7-c43a-db9fecd782c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/14\n",
            "----------\n",
            "train Loss: 1.3727 Acc: 0.5048\n",
            "val Loss: 1.2494 Acc: 0.5524\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 0.9724 Acc: 0.6551\n",
            "val Loss: 0.9548 Acc: 0.6703\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.7868 Acc: 0.7226\n",
            "val Loss: 0.9168 Acc: 0.6840\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.6471 Acc: 0.7715\n",
            "val Loss: 0.8598 Acc: 0.7113\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.5444 Acc: 0.8087\n",
            "val Loss: 0.8919 Acc: 0.7140\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.4485 Acc: 0.8413\n",
            "val Loss: 0.8461 Acc: 0.7339\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.3629 Acc: 0.8719\n",
            "val Loss: 0.8554 Acc: 0.7370\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.1498 Acc: 0.9538\n",
            "val Loss: 0.7750 Acc: 0.7788\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.0778 Acc: 0.9790\n",
            "val Loss: 0.8429 Acc: 0.7763\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.0439 Acc: 0.9905\n",
            "val Loss: 0.9351 Acc: 0.7722\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.0247 Acc: 0.9956\n",
            "val Loss: 1.0514 Acc: 0.7707\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.0134 Acc: 0.9979\n",
            "val Loss: 1.1644 Acc: 0.7683\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.0080 Acc: 0.9987\n",
            "val Loss: 1.2781 Acc: 0.7685\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.0044 Acc: 0.9995\n",
            "val Loss: 1.3852 Acc: 0.7688\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.9998\n",
            "val Loss: 1.3968 Acc: 0.7681\n",
            "\n",
            "Training complete in 5m 10s\n",
            "Best val Acc: 0.778800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts,StepLR\n",
        "def kl_loss_sym(x,y):\n",
        "    kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "    return kl_loss(nn.LogSoftmax(dim=-1)(x),y)\n",
        "\n",
        "def unlearning(\n",
        "        net,\n",
        "        retain_loader,\n",
        "        forget_loader,\n",
        "        val_loader,\n",
        "):\n",
        "    \"\"\"Simple unlearning by finetuning.\"\"\"\n",
        "    print('-----------------------------------')\n",
        "    epochs = 8\n",
        "    retain_bs = 256\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.005,\n",
        "                          momentum=0.9, weight_decay=0)\n",
        "    optimizer_retain = optim.SGD(net.parameters(), lr=0.001*retain_bs/64, momentum=0.9, weight_decay=1e-2)\n",
        "    ##the learning rate is associated with the batchsize we used\n",
        "    optimizer_forget = optim.SGD(net.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
        "    total_step = int(len(forget_loader)*epochs)\n",
        "    retain_ld = DataLoader(retain_loader.dataset, batch_size=retain_bs, shuffle=True)\n",
        "    retain_ld4fgt = DataLoader(retain_loader.dataset, batch_size=256, shuffle=True)\n",
        "    scheduler = CosineAnnealingLR(optimizer_forget, T_max=total_step, eta_min=1e-6)\n",
        "\n",
        "    net.train()\n",
        "    for sample in forget_loader: ##First Stage\n",
        "        inputs, output = sample\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        uniform_label = torch.ones_like(outputs).to(DEVICE) / outputs.shape[1] ##uniform pseudo label\n",
        "        loss = kl_loss_sym(outputs, uniform_label) ##optimize the distance between logits and pseudo labels\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    net.train()\n",
        "    for ep in range(epochs): ##Second Stage\n",
        "        net.train()\n",
        "        for sample_forget, sample_retain in zip(forget_loader, retain_ld4fgt):##Forget Round\n",
        "            t = 1.15 ##temperature coefficient\n",
        "            inputs_forget,inputs_retain = sample_forget[0],sample_retain[0]\n",
        "            inputs_forget, inputs_retain = inputs_forget.to(DEVICE), inputs_retain.to(DEVICE)\n",
        "            optimizer_forget.zero_grad()\n",
        "            outputs_forget,outputs_retain = net(inputs_forget),net(inputs_retain).detach()\n",
        "            loss = (-1 * nn.LogSoftmax(dim=-1)(outputs_forget @ outputs_retain.T/t)).mean() ##Contrastive Learning loss\n",
        "            loss.backward()\n",
        "            optimizer_forget.step()\n",
        "            scheduler.step()\n",
        "        for sample in retain_ld: ##Retain Round\n",
        "            inputs, labels = sample\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer_retain.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_retain.step()\n",
        "\n",
        "    print('-----------------------------------')\n",
        "    return net"
      ],
      "metadata": {
        "id": "h28_D54zQly7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model_ft_forget = unlearning(model_ft, retain_loader, forget_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omVPIY18TZri",
        "outputId": "380d01df-2adf-49ac-e37c-1c77cdea2383"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "-----------------------------------\n",
            "CPU times: user 2min 11s, sys: 959 ms, total: 2min 12s\n",
            "Wall time: 2min 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(forget_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHwTHdd8ThqF",
        "outputId": "f93b77e6-4c41-4992-a9d7-fb9866018bb1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smyceZBuTrGr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}