{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMN22w4MTbx-",
        "outputId": "6c959bd4-81e7-40d1-98e1-ca5d0d15fb8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: CUDA\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model, model_selection\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models import resnet18\n",
        "import numpy as np\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on device:\", DEVICE.upper())\n",
        "\n",
        "# manual random seed is used for dataset partitioning\n",
        "# to ensure reproducible results across runs\n",
        "RNG = torch.Generator().manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get data"
      ],
      "metadata": {
        "id": "oiz4Jx9STtGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CIFAR 10 dataset\n",
        "\n",
        "# Transformations\n",
        "normalize = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train data\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=normalize\n",
        ")\n",
        "# Train loader\n",
        "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "# Test data\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=normalize\n",
        ")\n",
        "# Test loader\n",
        "test_loader = DataLoader(test_set, batch_size=256, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5VyZKPgTsyD",
        "outputId": "a9b542f0-37ff-4a17-d52a-648634aac7b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29239963.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NykHdWHSTs2z",
        "outputId": "ec0666b6-c7c4-496e-dcf1-4e82f4fbf123"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose random forget indecies from some class\n",
        "# Index of class\n",
        "class_index = 1 # cars\n",
        "class_set = np.where(np.array(train_set.targets) == 1)[0]\n",
        "\n",
        "# Percantage of whole data ( from class )\n",
        "amount = 0.01 # 1 %\n",
        "amount_int = class_set.shape[0] * amount\n",
        "\n",
        "# Get indeces\n",
        "forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "# construct indices of retain from those of the forget set\n",
        "forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "forget_mask[forget_idx] = True\n",
        "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "# split train set into a forget and a retain set\n",
        "forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "# Generate forget and retain loaders\n",
        "forget_loader = torch.utils.data.DataLoader(\n",
        "    forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        ")\n",
        "retain_loader = torch.utils.data.DataLoader(\n",
        "    retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        ")"
      ],
      "metadata": {
        "id": "nbD4rJVLTrRh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add helping dicts\n",
        "\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\": test_loader\n",
        "}\n",
        "\n",
        "dataset_sizes = {\"train\": len(train_set), \"val\": len(test_set)}"
      ],
      "metadata": {
        "id": "TLN-G2k9T-nj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    with TemporaryDirectory() as tempdir:\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "        best_loss = 1000\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs = inputs.to(DEVICE)\n",
        "                    labels = labels.to(DEVICE)\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                if phase == 'train':\n",
        "                    scheduler.step()\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_loss < best_loss:\n",
        "                    best_loss = epoch_loss\n",
        "                    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val loss: {best_loss:4f}')\n",
        "\n",
        "        # load best model weights\n",
        "        model.load_state_dict(torch.load(best_model_params_path))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "3yWjdU4eUCjx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ResNet18\n",
        "\n",
        "model_train = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "model_train = model_train.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(model_train.parameters(), lr=0.001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "g6RsNI0EUaT7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model Resnet18 21 epochs\n",
        "model_train = train_model(model_train, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=21)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f94VGw_Ucil",
        "outputId": "938062ec-6fed-414d-ef35-b0b79b8acec7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/20\n",
            "----------\n",
            "train Loss: 1.3891 Acc: 0.5002\n",
            "val Loss: 1.2895 Acc: 0.5573\n",
            "\n",
            "Epoch 1/20\n",
            "----------\n",
            "train Loss: 0.9827 Acc: 0.6525\n",
            "val Loss: 1.0393 Acc: 0.6381\n",
            "\n",
            "Epoch 2/20\n",
            "----------\n",
            "train Loss: 0.7896 Acc: 0.7211\n",
            "val Loss: 0.9406 Acc: 0.6763\n",
            "\n",
            "Epoch 3/20\n",
            "----------\n",
            "train Loss: 0.6621 Acc: 0.7682\n",
            "val Loss: 0.8359 Acc: 0.7128\n",
            "\n",
            "Epoch 4/20\n",
            "----------\n",
            "train Loss: 0.5501 Acc: 0.8067\n",
            "val Loss: 0.8857 Acc: 0.7093\n",
            "\n",
            "Epoch 5/20\n",
            "----------\n",
            "train Loss: 0.4535 Acc: 0.8400\n",
            "val Loss: 1.0570 Acc: 0.6724\n",
            "\n",
            "Epoch 6/20\n",
            "----------\n",
            "train Loss: 0.3747 Acc: 0.8674\n",
            "val Loss: 0.9209 Acc: 0.7118\n",
            "\n",
            "Epoch 7/20\n",
            "----------\n",
            "train Loss: 0.1564 Acc: 0.9526\n",
            "val Loss: 0.7859 Acc: 0.7675\n",
            "\n",
            "Epoch 8/20\n",
            "----------\n",
            "train Loss: 0.0833 Acc: 0.9772\n",
            "val Loss: 0.8564 Acc: 0.7652\n",
            "\n",
            "Epoch 9/20\n",
            "----------\n",
            "train Loss: 0.0484 Acc: 0.9887\n",
            "val Loss: 0.9579 Acc: 0.7648\n",
            "\n",
            "Epoch 10/20\n",
            "----------\n",
            "train Loss: 0.0275 Acc: 0.9946\n",
            "val Loss: 1.0741 Acc: 0.7625\n",
            "\n",
            "Epoch 11/20\n",
            "----------\n",
            "train Loss: 0.0144 Acc: 0.9979\n",
            "val Loss: 1.2268 Acc: 0.7594\n",
            "\n",
            "Epoch 12/20\n",
            "----------\n",
            "train Loss: 0.0075 Acc: 0.9991\n",
            "val Loss: 1.3307 Acc: 0.7600\n",
            "\n",
            "Epoch 13/20\n",
            "----------\n",
            "train Loss: 0.0048 Acc: 0.9994\n",
            "val Loss: 1.4164 Acc: 0.7588\n",
            "\n",
            "Epoch 14/20\n",
            "----------\n",
            "train Loss: 0.0029 Acc: 0.9998\n",
            "val Loss: 1.4252 Acc: 0.7603\n",
            "\n",
            "Epoch 15/20\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.9999\n",
            "val Loss: 1.4224 Acc: 0.7611\n",
            "\n",
            "Epoch 16/20\n",
            "----------\n",
            "train Loss: 0.0022 Acc: 1.0000\n",
            "val Loss: 1.4308 Acc: 0.7622\n",
            "\n",
            "Epoch 17/20\n",
            "----------\n",
            "train Loss: 0.0021 Acc: 0.9999\n",
            "val Loss: 1.4380 Acc: 0.7616\n",
            "\n",
            "Epoch 18/20\n",
            "----------\n",
            "train Loss: 0.0020 Acc: 0.9999\n",
            "val Loss: 1.4558 Acc: 0.7623\n",
            "\n",
            "Epoch 19/20\n",
            "----------\n",
            "train Loss: 0.0017 Acc: 1.0000\n",
            "val Loss: 1.4592 Acc: 0.7616\n",
            "\n",
            "Epoch 20/20\n",
            "----------\n",
            "train Loss: 0.0016 Acc: 1.0000\n",
            "val Loss: 1.4624 Acc: 0.7622\n",
            "\n",
            "Training complete in 8m 30s\n",
            "Best val loss: 0.785851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save model weights\n",
        "torch.save(model_train.state_dict(), \"model_train_params_best_loss.pt\")"
      ],
      "metadata": {
        "id": "dTGQgso_UggP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check accuracy\n",
        "\n",
        "def accuracy(net, loader):\n",
        "    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "print(f\"Train set accuracy: {100.0 * accuracy(model_train, train_loader):0.1f}%\")\n",
        "print(f\"Test set accuracy: {100.0 * accuracy(model_train, test_loader):0.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8zYw0twW30H",
        "outputId": "33fada2a-1718-4631-f3d6-393f63893461"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set accuracy: 98.0%\n",
            "Test set accuracy: 76.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unlearning algorithm\n",
        "def unlearning(net, retain, forget, validation):\n",
        "    \"\"\"Unlearning by fine-tuning.\n",
        "\n",
        "    Fine-tuning is a very simple algorithm that trains using only\n",
        "    the retain set.\n",
        "\n",
        "    Args:\n",
        "      net : nn.Module.\n",
        "        pre-trained model to use as base of unlearning.\n",
        "      retain : torch.utils.data.DataLoader.\n",
        "        Dataset loader for access to the retain set. This is the subset\n",
        "        of the training set that we don't want to forget.\n",
        "      forget : torch.utils.data.DataLoader.\n",
        "        Dataset loader for access to the forget set. This is the subset\n",
        "        of the training set that we want to forget. This method doesn't\n",
        "        make use of the forget set.\n",
        "      validation : torch.utils.data.DataLoader.\n",
        "        Dataset loader for access to the validation set. This method doesn't\n",
        "        make use of the validation set.\n",
        "    Returns:\n",
        "      net : updated model\n",
        "    \"\"\"\n",
        "    epochs = 5\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    net.train()\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for inputs, targets in retain:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    net.eval()\n",
        "    return net"
      ],
      "metadata": {
        "id": "6wc09qONYMDE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_losses(net, loader):\n",
        "    \"\"\"Auxiliary function to compute per-sample losses\"\"\"\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    all_losses = []\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        logits = net(inputs)\n",
        "        losses = criterion(logits, targets).numpy(force=True)\n",
        "        for l in losses:\n",
        "            all_losses.append(l)\n",
        "\n",
        "    return np.array(all_losses)"
      ],
      "metadata": {
        "id": "KkBF3J-5YOwN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MIA score\n",
        "\n",
        "\n",
        "def simple_mia(sample_loss, members, n_splits=10, random_state=0):\n",
        "    \"\"\"Computes cross-validation score of a membership inference attack.\n",
        "\n",
        "    Args:\n",
        "      sample_loss : array_like of shape (n,).\n",
        "        objective function evaluated on n samples.\n",
        "      members : array_like of shape (n,),\n",
        "        whether a sample was used for training.\n",
        "      n_splits: int\n",
        "        number of splits to use in the cross-validation.\n",
        "    Returns:\n",
        "      scores : array_like of size (n_splits,)\n",
        "    \"\"\"\n",
        "\n",
        "    unique_members = np.unique(members)\n",
        "    if not np.all(unique_members == np.array([0, 1])):\n",
        "        raise ValueError(\"members should only have 0 and 1s\")\n",
        "\n",
        "    attack_model = linear_model.LogisticRegression()\n",
        "    cv = model_selection.StratifiedShuffleSplit(\n",
        "        n_splits=n_splits, random_state=random_state\n",
        "    )\n",
        "    return model_selection.cross_val_score(\n",
        "        attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\"\n",
        "    )"
      ],
      "metadata": {
        "id": "EjaFXOXbYTME"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unlearning algorithm\n",
        "def unlearning_by_epochs(net, retain, forget, validation, epochs=5):\n",
        "    \"\"\"Unlearning by fine-tuning.\n",
        "\n",
        "    Fine-tuning is a very simple algorithm that trains using only\n",
        "    the retain set.\n",
        "\n",
        "    Args:\n",
        "      net : nn.Module.\n",
        "        pre-trained model to use as base of unlearning.\n",
        "      retain : torch.utils.data.DataLoader.\n",
        "        Dataset loader for access to the retain set. This is the subset\n",
        "        of the training set that we don't want to forget.\n",
        "      forget : torch.utils.data.DataLoader.\n",
        "        Dataset loader for access to the forget set. This is the subset\n",
        "        of the training set that we want to forget. This method doesn't\n",
        "        make use of the forget set.\n",
        "      validation : torch.utils.data.DataLoader.\n",
        "        Dataset loader for access to the validation set. This method doesn't\n",
        "        make use of the validation set.\n",
        "    Returns:\n",
        "      net : updated model\n",
        "    \"\"\"\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    net.train()\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for inputs, targets in retain:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    net.eval()\n",
        "    return net"
      ],
      "metadata": {
        "id": "w3j59gBstFbI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = compute_losses(model_train, train_loader)\n",
        "test_losses = compute_losses(model_train, test_loader)\n",
        "\n",
        "# TRAIN MODEL\n",
        "# Get random test samples\n",
        "randomize = np.arange(len(test_losses))\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "# Compute forget losses on train dataset\n",
        "forget_losses = compute_losses(model_train, forget_loader)\n",
        "\n",
        "# Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "test_losses = test_losses[randomize][: len(forget_losses)]\n",
        "\n",
        "samples_mia = np.concatenate((test_losses, forget_losses)).reshape((-1, 1))\n",
        "labels_mia = [0] * len(test_losses) + [1] * len(forget_losses)\n",
        "\n",
        "mia_scores = simple_mia(samples_mia, labels_mia)\n",
        "\n",
        "print(\n",
        "    f\"The MIA has an accuracy of {mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtD2G9oKZPjP",
        "outputId": "c162fe74-cb75-494b-eb4b-f364a264d5a0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The MIA has an accuracy of 0.720 on forgotten vs unseen images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----- 1 PERCENT -------\")\n",
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "MIA_scores = []\n",
        "MIA_scores_retain = []\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "AD_score = []\n",
        "\n",
        "for class_num in classes:\n",
        "  # Define model from trained params\n",
        "\n",
        "  model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "  model_forget_ft.load_state_dict(torch.load(\"model_train_params_best_loss.pt\"))\n",
        "  model_forget_ft.to(DEVICE)\n",
        "\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == class_num)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount = 0.01 # 1 %\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning(model_forget_ft, retain_loader, forget_loader, test_loader)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "  # Retain model\n",
        "  # Add helping dicts\n",
        "  # MIA\n",
        "\n",
        "  # Compute forget losses for forget model\n",
        "  forget_losses_fr = compute_losses(model_ft_forget, forget_loader)\n",
        "  test_losses_fr = compute_losses(model_ft_forget, test_loader)\n",
        "\n",
        "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "  test_losses_fr = test_losses_fr[randomize][: len(forget_losses)]\n",
        "\n",
        "  # make sure we have a balanced dataset for the MIA\n",
        "  samples_mia_fr = np.concatenate((test_losses_fr, forget_losses_fr)).reshape((-1, 1))\n",
        "  labels_mia_fr = [0] * len(test_losses_fr) + [1] * len(forget_losses_fr)\n",
        "\n",
        "  mia_scores_fr = simple_mia(samples_mia_fr, labels_mia_fr)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr.mean():.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "\n",
        "MIA_scores = [m.mean() for m in MIA_scores]\n",
        "print(MIA_scores)\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)\n",
        "\n",
        "# Mean MIA\n",
        "mean_mia_1 = sum(MIA_scores) / len(MIA_scores)\n",
        "\n",
        "# Mean Accuracy Retain\n",
        "mean_accuracy_retain = sum(Accuracy_retain) / len(Accuracy_retain)\n",
        "mean_accuracy_test = sum(Accuracy_test) / len(Accuracy_test)\n",
        "\n",
        "# Mean Accuracy forget\n",
        "mean_accuracy_forget = sum(Accuracy_forget) / len(Accuracy_forget)\n",
        "\n",
        "print(\"MEANS\")\n",
        "\n",
        "print(mean_mia_1)\n",
        "print(mean_accuracy_retain)\n",
        "print(mean_accuracy_forget)\n",
        "print(mean_accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJYSFgoGYVt6",
        "outputId": "c44b7b5b-4f09-4bb6-e1dd-9e63e2befd46"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- 1 PERCENT -------\n",
            "Retain set accuracy FORGET model: 99.4%\n",
            "Test set accuracy FORGET model: 76.3%\n",
            "Forget set accuracy FORGET model: 90.0%\n",
            "The MIA has an accuracy of 0.560 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.7%\n",
            "Test set accuracy FORGET model: 76.3%\n",
            "Forget set accuracy FORGET model: 94.0%\n",
            "The MIA has an accuracy of 0.610 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.7%\n",
            "Test set accuracy FORGET model: 77.0%\n",
            "Forget set accuracy FORGET model: 78.0%\n",
            "The MIA has an accuracy of 0.560 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.5%\n",
            "Test set accuracy FORGET model: 76.2%\n",
            "Forget set accuracy FORGET model: 76.0%\n",
            "The MIA has an accuracy of 0.420 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.2%\n",
            "Forget set accuracy FORGET model: 92.0%\n",
            "The MIA has an accuracy of 0.520 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 77.2%\n",
            "Forget set accuracy FORGET model: 84.0%\n",
            "The MIA has an accuracy of 0.610 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 77.2%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.570 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.5%\n",
            "Test set accuracy FORGET model: 76.5%\n",
            "Forget set accuracy FORGET model: 84.0%\n",
            "The MIA has an accuracy of 0.570 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.1%\n",
            "Forget set accuracy FORGET model: 94.0%\n",
            "The MIA has an accuracy of 0.550 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.7%\n",
            "Test set accuracy FORGET model: 76.6%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.590 on forgotten vs unseen images on FORGET model\n",
            "[0.5599999999999999, 0.61, 0.5599999999999999, 0.42000000000000004, 0.52, 0.6100000000000001, 0.57, 0.5700000000000001, 0.55, 0.5900000000000001]\n",
            "[99.44145262357111, 99.71171171171171, 99.74374887389642, 99.54154154154155, 99.81982342695842, 99.77977977977977, 99.82782782782783, 99.52352352352352, 99.75775775775776, 99.74974974974975]\n",
            "[90.0, 94.0, 78.0, 76.0, 92.0, 84.0, 86.0, 84.0, 94.0, 86.0]\n",
            "[76.3, 76.27000000000001, 76.98, 76.23, 76.19, 77.17, 77.16, 76.49000000000001, 76.1, 76.64]\n",
            "MEANS\n",
            "0.5559999999999999\n",
            "99.68969168163179\n",
            "86.4\n",
            "76.55300000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---- 2 PERCANTAGES -----\")\n",
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "MIA_scores = []\n",
        "MIA_scores_retain = []\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "AD_score = []\n",
        "\n",
        "for class_num in classes:\n",
        "  # Define model from trained params\n",
        "\n",
        "  model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "  model_forget_ft.load_state_dict(torch.load(\"model_train_params_best_loss.pt\"))\n",
        "  model_forget_ft.to(DEVICE)\n",
        "\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == class_num)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount = 0.02 # 1 %\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning(model_forget_ft, retain_loader, forget_loader, test_loader)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "  # Retain model\n",
        "  # Add helping dicts\n",
        "  # MIA\n",
        "\n",
        "  # Compute forget losses for forget model\n",
        "  forget_losses_fr = compute_losses(model_ft_forget, forget_loader)\n",
        "  test_losses_fr = compute_losses(model_ft_forget, test_loader)\n",
        "\n",
        "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "  test_losses_fr = test_losses_fr[randomize][: len(forget_losses)]\n",
        "\n",
        "  # make sure we have a balanced dataset for the MIA\n",
        "  samples_mia_fr = np.concatenate((test_losses_fr, forget_losses_fr)).reshape((-1, 1))\n",
        "  labels_mia_fr = [0] * len(test_losses_fr) + [1] * len(forget_losses_fr)\n",
        "\n",
        "  mia_scores_fr = simple_mia(samples_mia_fr, labels_mia_fr)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr.mean():.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "\n",
        "MIA_scores = [m.mean() for m in MIA_scores]\n",
        "print(MIA_scores)\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)\n",
        "\n",
        "# Mean MIA\n",
        "mean_mia_1 = sum(MIA_scores) / len(MIA_scores)\n",
        "\n",
        "# Mean Accuracy Retain\n",
        "mean_accuracy_retain = sum(Accuracy_retain) / len(Accuracy_retain)\n",
        "mean_accuracy_test = sum(Accuracy_test) / len(Accuracy_test)\n",
        "\n",
        "# Mean Accuracy forget\n",
        "mean_accuracy_forget = sum(Accuracy_forget) / len(Accuracy_forget)\n",
        "\n",
        "print(\"MEANS\")\n",
        "\n",
        "print(mean_mia_1)\n",
        "print(mean_accuracy_retain)\n",
        "print(mean_accuracy_forget)\n",
        "print(mean_accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mac5VywIfiQL",
        "outputId": "1386a364-b499-44fa-d00c-93ab79337c9b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- 2 PERCANTAGES -----\n",
            "Retain set accuracy FORGET model: 99.7%\n",
            "Test set accuracy FORGET model: 76.3%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.753 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.2%\n",
            "Test set accuracy FORGET model: 76.4%\n",
            "Forget set accuracy FORGET model: 92.0%\n",
            "The MIA has an accuracy of 0.687 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.7%\n",
            "Forget set accuracy FORGET model: 80.0%\n",
            "The MIA has an accuracy of 0.667 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.8%\n",
            "Forget set accuracy FORGET model: 74.0%\n",
            "The MIA has an accuracy of 0.667 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.9%\n",
            "Test set accuracy FORGET model: 76.8%\n",
            "Forget set accuracy FORGET model: 84.0%\n",
            "The MIA has an accuracy of 0.700 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.6%\n",
            "Test set accuracy FORGET model: 76.1%\n",
            "Forget set accuracy FORGET model: 75.0%\n",
            "The MIA has an accuracy of 0.640 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.9%\n",
            "Forget set accuracy FORGET model: 85.0%\n",
            "The MIA has an accuracy of 0.653 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.6%\n",
            "Test set accuracy FORGET model: 76.3%\n",
            "Forget set accuracy FORGET model: 83.0%\n",
            "The MIA has an accuracy of 0.673 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 98.4%\n",
            "Test set accuracy FORGET model: 75.5%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.640 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.5%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.660 on forgotten vs unseen images on FORGET model\n",
            "[0.7533333333333334, 0.6866666666666668, 0.6666666666666667, 0.6666666666666667, 0.7, 0.64, 0.6533333333333333, 0.6733333333333333, 0.6399999999999999, 0.6599999999999999]\n",
            "[99.69140131855801, 99.20240480961924, 99.80761908578987, 99.76353179295005, 99.86372745490982, 99.62726197871787, 99.77155682023124, 99.60924192934293, 98.42694827966254, 99.81563865175744]\n",
            "[86.0, 92.0, 80.0, 74.0, 84.0, 75.0, 85.0, 83.0, 86.0, 86.0]\n",
            "[76.28, 76.42, 76.7, 76.77000000000001, 76.85, 76.09, 76.92, 76.3, 75.5, 76.51]\n",
            "MEANS\n",
            "0.6739999999999999\n",
            "99.55793321215391\n",
            "83.1\n",
            "76.434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---- 5 PERCANTAGES -----\")\n",
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "MIA_scores = []\n",
        "MIA_scores_retain = []\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "AD_score = []\n",
        "\n",
        "for class_num in classes:\n",
        "  # Define model from trained params\n",
        "\n",
        "  model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "  model_forget_ft.load_state_dict(torch.load(\"model_train_params_best_loss.pt\"))\n",
        "  model_forget_ft.to(DEVICE)\n",
        "\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == class_num)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount = 0.05 # 1 %\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning(model_forget_ft, retain_loader, forget_loader, test_loader)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "  # Retain model\n",
        "  # Add helping dicts\n",
        "  # MIA\n",
        "\n",
        "  # Compute forget losses for forget model\n",
        "  forget_losses_fr = compute_losses(model_ft_forget, forget_loader)\n",
        "  test_losses_fr = compute_losses(model_ft_forget, test_loader)\n",
        "\n",
        "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "  test_losses_fr = test_losses_fr[randomize][: len(forget_losses)]\n",
        "\n",
        "  # make sure we have a balanced dataset for the MIA\n",
        "  samples_mia_fr = np.concatenate((test_losses_fr, forget_losses_fr)).reshape((-1, 1))\n",
        "  labels_mia_fr = [0] * len(test_losses_fr) + [1] * len(forget_losses_fr)\n",
        "\n",
        "  mia_scores_fr = simple_mia(samples_mia_fr, labels_mia_fr)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr.mean():.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "\n",
        "MIA_scores = [m.mean() for m in MIA_scores]\n",
        "print(MIA_scores)\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)\n",
        "\n",
        "# Mean MIA\n",
        "mean_mia_1 = sum(MIA_scores) / len(MIA_scores)\n",
        "\n",
        "# Mean Accuracy Retain\n",
        "mean_accuracy_retain = sum(Accuracy_retain) / len(Accuracy_retain)\n",
        "mean_accuracy_test = sum(Accuracy_test) / len(Accuracy_test)\n",
        "\n",
        "# Mean Accuracy forget\n",
        "mean_accuracy_forget = sum(Accuracy_forget) / len(Accuracy_forget)\n",
        "\n",
        "print(\"MEANS\")\n",
        "\n",
        "print(mean_mia_1)\n",
        "print(mean_accuracy_retain)\n",
        "print(mean_accuracy_forget)\n",
        "print(mean_accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQdsk6KTfl9q",
        "outputId": "7bf014db-4cf2-488d-957b-6964ba08df2e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- 5 PERCANTAGES -----\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.9%\n",
            "Forget set accuracy FORGET model: 87.2%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 77.0%\n",
            "Forget set accuracy FORGET model: 92.0%\n",
            "The MIA has an accuracy of 0.830 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.3%\n",
            "Test set accuracy FORGET model: 76.1%\n",
            "Forget set accuracy FORGET model: 76.0%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.9%\n",
            "Test set accuracy FORGET model: 77.2%\n",
            "Forget set accuracy FORGET model: 74.0%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.7%\n",
            "Test set accuracy FORGET model: 76.6%\n",
            "Forget set accuracy FORGET model: 84.4%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.9%\n",
            "Test set accuracy FORGET model: 76.8%\n",
            "Forget set accuracy FORGET model: 81.2%\n",
            "The MIA has an accuracy of 0.837 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.2%\n",
            "Test set accuracy FORGET model: 76.3%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 77.2%\n",
            "Forget set accuracy FORGET model: 89.6%\n",
            "The MIA has an accuracy of 0.867 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.5%\n",
            "Test set accuracy FORGET model: 76.1%\n",
            "Forget set accuracy FORGET model: 94.0%\n",
            "The MIA has an accuracy of 0.847 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.6%\n",
            "Forget set accuracy FORGET model: 92.0%\n",
            "The MIA has an accuracy of 0.830 on forgotten vs unseen images on FORGET model\n",
            "[0.8333333333333334, 0.8299999999999998, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8366666666666667, 0.8333333333333334, 0.8666666666666666, 0.8466666666666667, 0.8299999999999998]\n",
            "[99.78695608481559, 99.8271461017426, 99.34484213911051, 99.88946501065156, 99.72061986211887, 99.8995096068816, 99.18395240392346, 99.82917663136317, 99.45731915663377, 99.77490151941475]\n",
            "[87.2, 92.0, 76.0, 74.0, 84.39999999999999, 81.2, 86.0, 89.60000000000001, 94.0, 92.0]\n",
            "[76.86, 76.96, 76.07000000000001, 77.18, 76.59, 76.84, 76.29, 77.17, 76.08, 76.55999999999999]\n",
            "MEANS\n",
            "0.8376666666666666\n",
            "99.67138885166557\n",
            "85.64\n",
            "76.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---- 10 % -----\")\n",
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "MIA_scores = []\n",
        "MIA_scores_retain = []\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "AD_score = []\n",
        "\n",
        "for class_num in classes:\n",
        "  # Define model from trained params\n",
        "\n",
        "  model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "  model_forget_ft.load_state_dict(torch.load(\"model_train_params_best_loss.pt\"))\n",
        "  model_forget_ft.to(DEVICE)\n",
        "\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == class_num)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount = 0.1 # 1 %\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning(model_forget_ft, retain_loader, forget_loader, test_loader)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "  # Retain model\n",
        "  # Add helping dicts\n",
        "  # MIA\n",
        "\n",
        "  # Compute forget losses for forget model\n",
        "  forget_losses_fr = compute_losses(model_ft_forget, forget_loader)\n",
        "  test_losses_fr = compute_losses(model_ft_forget, test_loader)\n",
        "\n",
        "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "  test_losses_fr = test_losses_fr[randomize][: len(forget_losses)]\n",
        "\n",
        "  # make sure we have a balanced dataset for the MIA\n",
        "  samples_mia_fr = np.concatenate((test_losses_fr, forget_losses_fr)).reshape((-1, 1))\n",
        "  labels_mia_fr = [0] * len(test_losses_fr) + [1] * len(forget_losses_fr)\n",
        "\n",
        "  mia_scores_fr = simple_mia(samples_mia_fr, labels_mia_fr)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr.mean():.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "\n",
        "MIA_scores = [m.mean() for m in MIA_scores]\n",
        "print(MIA_scores)\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)\n",
        "\n",
        "# Mean MIA\n",
        "mean_mia_1 = sum(MIA_scores) / len(MIA_scores)\n",
        "\n",
        "# Mean Accuracy Retain\n",
        "mean_accuracy_retain = sum(Accuracy_retain) / len(Accuracy_retain)\n",
        "mean_accuracy_test = sum(Accuracy_test) / len(Accuracy_test)\n",
        "\n",
        "# Mean Accuracy forget\n",
        "mean_accuracy_forget = sum(Accuracy_forget) / len(Accuracy_forget)\n",
        "\n",
        "print(\"MEANS\")\n",
        "\n",
        "print(mean_mia_1)\n",
        "print(mean_accuracy_retain)\n",
        "print(mean_accuracy_forget)\n",
        "print(mean_accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bBQIyMflpv8",
        "outputId": "63b0b2b7-60f2-4510-b33d-d9d1d3be23cf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- 10 % -----\n",
            "Retain set accuracy FORGET model: 99.9%\n",
            "Test set accuracy FORGET model: 77.0%\n",
            "Forget set accuracy FORGET model: 87.8%\n",
            "The MIA has an accuracy of 0.911 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.9%\n",
            "Test set accuracy FORGET model: 76.9%\n",
            "Forget set accuracy FORGET model: 90.2%\n",
            "The MIA has an accuracy of 0.916 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.2%\n",
            "Forget set accuracy FORGET model: 81.2%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.7%\n",
            "Forget set accuracy FORGET model: 74.2%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.7%\n",
            "Test set accuracy FORGET model: 76.8%\n",
            "Forget set accuracy FORGET model: 80.2%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.7%\n",
            "Test set accuracy FORGET model: 76.5%\n",
            "Forget set accuracy FORGET model: 77.2%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.2%\n",
            "Test set accuracy FORGET model: 75.8%\n",
            "Forget set accuracy FORGET model: 88.2%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.7%\n",
            "Forget set accuracy FORGET model: 88.8%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.7%\n",
            "Forget set accuracy FORGET model: 92.2%\n",
            "The MIA has an accuracy of 0.907 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model: 99.8%\n",
            "Test set accuracy FORGET model: 76.4%\n",
            "Forget set accuracy FORGET model: 88.8%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "[0.9109090909090908, 0.9163636363636363, 0.909090909090909, 0.909090909090909, 0.909090909090909, 0.909090909090909, 0.909090909090909, 0.909090909090909, 0.9072727272727272, 0.909090909090909]\n",
            "[99.89904292694746, 99.86068767792607, 99.77382418869524, 99.826346821743, 99.67489853199524, 99.73949919224556, 99.15201195259343, 99.78596668349319, 99.82635032811712, 99.84249742543868]\n",
            "[87.8, 90.2, 81.2, 74.2, 80.2, 77.2, 88.2, 88.8, 92.2, 88.8]\n",
            "[77.0, 76.88000000000001, 76.18, 76.73, 76.84, 76.47, 75.84, 76.68, 76.74, 76.41]\n",
            "MEANS\n",
            "0.9098181818181817\n",
            "99.73811257291949\n",
            "84.88\n",
            "76.57700000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "MIA_scores = []\n",
        "MIA_scores_retain = []\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "AD_score = []\n",
        "# Define model from trained params\n",
        "\n",
        "model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "model_forget_ft.load_state_dict(torch.load(\"model_train_params_best_loss.pt\"))\n",
        "model_forget_ft.to(DEVICE)\n",
        "\n",
        "for epoch_num in range(10):\n",
        "\n",
        "\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == 1)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount = 0.02 # 2 %\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning_by_epochs(model_forget_ft, retain_loader, forget_loader, test_loader, 1)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model epoch {epoch_num}: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model epoch {epoch_num}: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model {epoch_num}:: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "\n",
        "  # Retain model\n",
        "  # Add helping dicts\n",
        "  # MIA\n",
        "\n",
        "  # Compute forget losses for forget model\n",
        "  forget_losses_fr = compute_losses(model_ft_forget, forget_loader)\n",
        "  test_losses_fr = compute_losses(model_ft_forget, test_loader)\n",
        "\n",
        "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "  test_losses_fr = test_losses_fr[randomize][: len(forget_losses)]\n",
        "\n",
        "  # make sure we have a balanced dataset for the MIA\n",
        "  samples_mia_fr = np.concatenate((test_losses_fr, forget_losses_fr)).reshape((-1, 1))\n",
        "  labels_mia_fr = [0] * len(test_losses_fr) + [1] * len(forget_losses_fr)\n",
        "\n",
        "  mia_scores_fr = simple_mia(samples_mia_fr, labels_mia_fr)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr.mean():.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "\n",
        "MIA_scores = [m.mean() for m in MIA_scores]\n",
        "print(MIA_scores)\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)\n",
        "\n",
        "# Mean MIA\n",
        "mean_mia_1 = sum(MIA_scores) / len(MIA_scores)\n",
        "\n",
        "# Mean Accuracy Retain\n",
        "mean_accuracy_retain = sum(Accuracy_retain) / len(Accuracy_retain)\n",
        "mean_accuracy_test = sum(Accuracy_test) / len(Accuracy_test)\n",
        "\n",
        "# Mean Accuracy forget\n",
        "mean_accuracy_forget = sum(Accuracy_forget) / len(Accuracy_forget)\n",
        "\n",
        "print(\"MEANS\")\n",
        "\n",
        "print(mean_mia_1)\n",
        "print(mean_accuracy_retain)\n",
        "print(mean_accuracy_forget)\n",
        "print(mean_accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvML-PLBqpsq",
        "outputId": "87174b2c-cae1-4abd-8653-b65d7ef0c01f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retain set accuracy FORGET model epoch 0: 81.6%\n",
            "Test set accuracy FORGET model epoch 0: 70.8%\n",
            "Forget set accuracy FORGET model 0:: 81.0%\n",
            "The MIA has an accuracy of 0.667 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 1: 85.1%\n",
            "Test set accuracy FORGET model epoch 1: 72.0%\n",
            "Forget set accuracy FORGET model 1:: 95.0%\n",
            "The MIA has an accuracy of 0.767 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 2: 87.0%\n",
            "Test set accuracy FORGET model epoch 2: 71.5%\n",
            "Forget set accuracy FORGET model 2:: 81.0%\n",
            "The MIA has an accuracy of 0.667 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 3: 89.2%\n",
            "Test set accuracy FORGET model epoch 3: 72.9%\n",
            "Forget set accuracy FORGET model 3:: 85.0%\n",
            "The MIA has an accuracy of 0.667 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 4: 87.8%\n",
            "Test set accuracy FORGET model epoch 4: 71.9%\n",
            "Forget set accuracy FORGET model 4:: 83.0%\n",
            "The MIA has an accuracy of 0.640 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 5: 89.8%\n",
            "Test set accuracy FORGET model epoch 5: 72.2%\n",
            "Forget set accuracy FORGET model 5:: 97.0%\n",
            "The MIA has an accuracy of 0.687 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 6: 88.6%\n",
            "Test set accuracy FORGET model epoch 6: 71.3%\n",
            "Forget set accuracy FORGET model 6:: 90.0%\n",
            "The MIA has an accuracy of 0.693 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 7: 88.5%\n",
            "Test set accuracy FORGET model epoch 7: 70.1%\n",
            "Forget set accuracy FORGET model 7:: 93.0%\n",
            "The MIA has an accuracy of 0.707 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 8: 88.7%\n",
            "Test set accuracy FORGET model epoch 8: 71.1%\n",
            "Forget set accuracy FORGET model 8:: 86.0%\n",
            "The MIA has an accuracy of 0.647 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 9: 89.2%\n",
            "Test set accuracy FORGET model epoch 9: 70.8%\n",
            "Forget set accuracy FORGET model 9:: 88.0%\n",
            "The MIA has an accuracy of 0.660 on forgotten vs unseen images on FORGET model\n",
            "[0.6666666666666667, 0.7666666666666667, 0.6666666666666667, 0.6666666666666667, 0.6399999999999999, 0.6866666666666666, 0.6933333333333334, 0.7066666666666667, 0.6466666666666667, 0.6599999999999999]\n",
            "[81.64966633935191, 85.10049898799623, 87.01707277973709, 89.24291611558655, 87.84168336673346, 89.7639373171416, 88.60721442885772, 88.48496993987976, 88.71743486973948, 89.24091218788827]\n",
            "[81.0, 95.0, 81.0, 85.0, 83.0, 97.0, 90.0, 93.0, 86.0, 88.0]\n",
            "[70.78, 72.02, 71.55, 72.89999999999999, 71.87, 72.21, 71.26, 70.11, 71.11, 70.83]\n",
            "MEANS\n",
            "0.68\n",
            "87.56663063329121\n",
            "87.9\n",
            "71.464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9F7cNNckviux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}