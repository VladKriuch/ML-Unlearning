{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMN22w4MTbx-",
        "outputId": "59c62c66-0f54-4c32-efd5-0c030b27043f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: CUDA\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model, model_selection\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models import resnet18\n",
        "import numpy as np\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on device:\", DEVICE.upper())\n",
        "\n",
        "# manual random seed is used for dataset partitioning\n",
        "# to ensure reproducible results across runs\n",
        "RNG = torch.Generator().manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get data"
      ],
      "metadata": {
        "id": "oiz4Jx9STtGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CIFAR 10 dataset\n",
        "\n",
        "# Transformations\n",
        "normalize = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train data\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=normalize\n",
        ")\n",
        "# Train loader\n",
        "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "# Test data\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=normalize\n",
        ")\n",
        "# Test loader\n",
        "test_loader = DataLoader(test_set, batch_size=256, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5VyZKPgTsyD",
        "outputId": "2dd950f2-3068-4e19-9ec4-38d4f6fcea01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12309985.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NykHdWHSTs2z",
        "outputId": "70f17277-805e-4e77-c6de-c92d6e254e34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose random forget indecies from some class\n",
        "# Index of class\n",
        "class_index = 1 # cars\n",
        "class_set = np.where(np.array(train_set.targets) == 1)[0]\n",
        "\n",
        "# Percantage of whole data ( from class )\n",
        "amount = 0.01 # 1 %\n",
        "amount_int = class_set.shape[0] * amount\n",
        "\n",
        "# Get indeces\n",
        "forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "# construct indices of retain from those of the forget set\n",
        "forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "forget_mask[forget_idx] = True\n",
        "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "# split train set into a forget and a retain set\n",
        "forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "# Generate forget and retain loaders\n",
        "forget_loader = torch.utils.data.DataLoader(\n",
        "    forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        ")\n",
        "retain_loader = torch.utils.data.DataLoader(\n",
        "    retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        ")"
      ],
      "metadata": {
        "id": "nbD4rJVLTrRh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add helping dicts\n",
        "\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\": test_loader\n",
        "}\n",
        "\n",
        "dataset_sizes = {\"train\": len(train_set), \"val\": len(test_set)}"
      ],
      "metadata": {
        "id": "TLN-G2k9T-nj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    with TemporaryDirectory() as tempdir:\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "        best_loss = 1000\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs = inputs.to(DEVICE)\n",
        "                    labels = labels.to(DEVICE)\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                if phase == 'train':\n",
        "                    scheduler.step()\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_loss < best_loss:\n",
        "                    best_loss = epoch_loss\n",
        "                    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val loss: {best_loss:4f}')\n",
        "\n",
        "        # load best model weights\n",
        "        model.load_state_dict(torch.load(best_model_params_path))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "3yWjdU4eUCjx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ResNet18\n",
        "\n",
        "model_train = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "model_train = model_train.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(model_train.parameters(), lr=0.001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "g6RsNI0EUaT7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_train.load_state_dict(torch.load(\"model_train_params_best_loss.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f94VGw_Ucil",
        "outputId": "5d95be16-2889-4397-9ec7-61e17767c897"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check accuracy\n",
        "\n",
        "def accuracy(net, loader):\n",
        "    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "print(f\"Train set accuracy: {100.0 * accuracy(model_train, train_loader):0.1f}%\")\n",
        "print(f\"Test set accuracy: {100.0 * accuracy(model_train, test_loader):0.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8zYw0twW30H",
        "outputId": "554bb46e-2642-4789-aecc-b3f2a612c31f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set accuracy: 97.8%\n",
            "Test set accuracy: 76.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, Subset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "3Wqb0aKJ5FQA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sch = 'linear'\n",
        "init_rate = 0.3\n",
        "init_method = 'snip_little_grad'\n",
        "lr = 0.001\n",
        "epoch = 5\n",
        "weight_decay = 5e-4\n",
        "\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class LinearAnnealingLR(_LRScheduler):\n",
        "    def __init__(self, optimizer, num_annealing_steps, num_total_steps):\n",
        "        self.num_annealing_steps = num_annealing_steps\n",
        "        self.num_total_steps = num_total_steps\n",
        "\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self._step_count <= self.num_annealing_steps:\n",
        "            return [base_lr * self._step_count / self.num_annealing_steps for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr * (self.num_total_steps - self._step_count) / (self.num_total_steps - self.num_annealing_steps) for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "def set_layer(model, layer_name, layer):\n",
        "    splited = layer_name.split('.')\n",
        "    if len(splited) == 1:\n",
        "        setattr(model, splited[0], layer)\n",
        "    elif len(splited) == 3:\n",
        "        setattr(getattr(model, splited[0])[int(splited[1])], splited[2], layer)\n",
        "    elif len(splited) == 4:\n",
        "        getattr(getattr(model, splited[0])[int(splited[1])], splited[2])[int(splited[3])] = layer\n",
        "\n",
        "class Masker(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, mask):\n",
        "        ctx.save_for_backward(mask)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad):\n",
        "        mask, = ctx.saved_tensors\n",
        "        return grad * mask, None\n",
        "\n",
        "class MaskConv2d(nn.Conv2d):\n",
        "    def __init__(self, mask, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device='cpu'):\n",
        "        super(MaskConv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
        "                                     padding, dilation, groups, bias, padding_mode, device=device)\n",
        "        self.mask = mask\n",
        "\n",
        "    def forward(self, input):\n",
        "        masked_weight = Masker.apply(self.weight, self.mask)\n",
        "        return super(MaskConv2d, self)._conv_forward(input, masked_weight, self.bias)\n",
        "\n",
        "@torch.no_grad()\n",
        "def replace_maskconv(model):\n",
        "    print(\"Remove Maskconv\")\n",
        "    for name, m in list(model.named_modules()):\n",
        "        if isinstance(m, MaskConv2d):\n",
        "            conv = nn.Conv2d(m.in_channels, m.out_channels, m.kernel_size, m.stride,\n",
        "                 m.padding, m.dilation, m.groups, m.bias!=None, m.padding_mode, device=DEVICE)\n",
        "            conv.weight.data = m.weight\n",
        "            conv.bias = m.bias\n",
        "            set_layer(model, name, conv)\n",
        "\n",
        "def get_grads_for_snip(model, retain_loader, forget_loader):\n",
        "    indices = torch.randperm(len(retain_loader.dataset), dtype=torch.int32, device='cpu')[:len(forget_loader.dataset)]\n",
        "    retain_dataset = Subset(retain_loader.dataset, indices)\n",
        "    retain_loader = DataLoader(retain_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    model.zero_grad()\n",
        "    for sample in retain_loader:\n",
        "        inputs = sample[0]\n",
        "        targets = sample[1]\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "    for sample in forget_loader:\n",
        "        inputs = sample[0]\n",
        "        targets = sample[1]\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = -F.cross_entropy(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "@torch.no_grad()\n",
        "def re_init_model_snip_ver2_little_grad(model, px): # re init smallest gradients\n",
        "    print(\"Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\")\n",
        "    for name, m in list(model.named_modules()):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            mask = torch.zeros_like(m.weight, device=DEVICE).bool()\n",
        "            nparams_toprune = round(px*mask.nelement())\n",
        "\n",
        "            out_c, in_c, ke, _ = mask.shape\n",
        "            value = -m.weight.grad.abs()\n",
        "            topk = torch.topk(value.view(-1), k=nparams_toprune)\n",
        "            mask.view(-1)[topk.indices] = True\n",
        "            grad_mask = mask.clone().float()\n",
        "            grad_mask[grad_mask==0] += 0.1\n",
        "\n",
        "            new_conv = MaskConv2d(grad_mask, m.in_channels, m.out_channels, m.kernel_size, m.stride,\n",
        "                 m.padding, m.dilation, m.groups, m.bias!=None, m.padding_mode, device=DEVICE)\n",
        "            nn.init.kaiming_normal_(new_conv.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "            new_conv.weight.data[~mask] = m.weight[~mask]\n",
        "\n",
        "            set_layer(model, name, new_conv)\n",
        "\n",
        "\n",
        "def unlearning(\n",
        "    net,\n",
        "    retain_loader,\n",
        "    forget_loader,\n",
        "    val_loader):\n",
        "    if init_method=='snip_little_grad':\n",
        "        replace_maskconv(net)\n",
        "        get_grads_for_snip(net, retain_loader, forget_loader)\n",
        "        re_init_model_snip_ver2_little_grad(net, init_rate)\n",
        "\n",
        "    \"\"\"Simple unlearning by finetuning.\"\"\"\n",
        "    epochs = epoch\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr,\n",
        "                      momentum=0.9, weight_decay=weight_decay)\n",
        "\n",
        "    if sch=='linear':\n",
        "        scheduler = LinearAnnealingLR(optimizer, num_annealing_steps=(epochs+1)//2, num_total_steps=epochs+1)\n",
        "    net.train()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        for sample in retain_loader:\n",
        "            inputs = sample[0]\n",
        "            targets = sample[1]\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "    #remove_prune(net)\n",
        "    net.eval()\n",
        "    return net"
      ],
      "metadata": {
        "id": "6wc09qONYMDE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_losses(net, loader):\n",
        "    \"\"\"Auxiliary function to compute per-sample losses\"\"\"\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    all_losses = []\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        logits = net(inputs)\n",
        "        losses = criterion(logits, targets).numpy(force=True)\n",
        "        for l in losses:\n",
        "            all_losses.append(l)\n",
        "\n",
        "    return np.array(all_losses)"
      ],
      "metadata": {
        "id": "KkBF3J-5YOwN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MIA score\n",
        "\n",
        "\n",
        "def simple_mia(sample_loss, members, n_splits=10, random_state=0):\n",
        "    \"\"\"Computes cross-validation score of a membership inference attack.\n",
        "\n",
        "    Args:\n",
        "      sample_loss : array_like of shape (n,).\n",
        "        objective function evaluated on n samples.\n",
        "      members : array_like of shape (n,),\n",
        "        whether a sample was used for training.\n",
        "      n_splits: int\n",
        "        number of splits to use in the cross-validation.\n",
        "    Returns:\n",
        "      scores : array_like of size (n_splits,)\n",
        "    \"\"\"\n",
        "\n",
        "    unique_members = np.unique(members)\n",
        "    if not np.all(unique_members == np.array([0, 1])):\n",
        "        raise ValueError(\"members should only have 0 and 1s\")\n",
        "\n",
        "    attack_model = linear_model.LogisticRegression()\n",
        "    cv = model_selection.StratifiedShuffleSplit(\n",
        "        n_splits=n_splits, random_state=random_state\n",
        "    )\n",
        "    return model_selection.cross_val_score(\n",
        "        attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\"\n",
        "    )"
      ],
      "metadata": {
        "id": "EjaFXOXbYTME"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = compute_losses(model_train, train_loader)\n",
        "test_losses = compute_losses(model_train, test_loader)\n",
        "\n",
        "# TRAIN MODEL\n",
        "# Get random test samples\n",
        "randomize = np.arange(len(test_losses))\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "# Compute forget losses on train dataset\n",
        "forget_losses = compute_losses(model_train, forget_loader)\n",
        "\n",
        "# Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "test_losses = test_losses[randomize][: len(forget_losses)]\n",
        "\n",
        "samples_mia = np.concatenate((test_losses, forget_losses)).reshape((-1, 1))\n",
        "labels_mia = [0] * len(test_losses) + [1] * len(forget_losses)\n",
        "\n",
        "mia_scores = simple_mia(samples_mia, labels_mia)\n",
        "\n",
        "print(\n",
        "    f\"The MIA has an accuracy of {mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtD2G9oKZPjP",
        "outputId": "c38588dd-cd98-4cf5-9632-ad4c5c41b020"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The MIA has an accuracy of 0.870 on forgotten vs unseen images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----- 1 PERCENT -------\")\n",
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "MIA_scores = []\n",
        "MIA_scores_retain = []\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "AD_score = []\n",
        "\n",
        "for class_num in classes:\n",
        "  # Define model from trained params\n",
        "\n",
        "  model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "  model_forget_ft.load_state_dict(torch.load(\"model_train_params_best_loss.pt\"))\n",
        "  model_forget_ft.to(DEVICE)\n",
        "\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == class_num)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount = 0.01 # 1 %\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning(model_forget_ft, retain_loader, forget_loader, test_loader)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "  # Retain model\n",
        "  # Add helping dicts\n",
        "  # MIA\n",
        "\n",
        "  # Compute forget losses for forget model\n",
        "  forget_losses_fr = compute_losses(model_ft_forget, forget_loader)\n",
        "  test_losses_fr = compute_losses(model_ft_forget, test_loader)\n",
        "\n",
        "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "  test_losses_fr = test_losses_fr[randomize][: len(forget_losses)]\n",
        "\n",
        "  # make sure we have a balanced dataset for the MIA\n",
        "  samples_mia_fr = np.concatenate((test_losses_fr, forget_losses_fr)).reshape((-1, 1))\n",
        "  labels_mia_fr = [0] * len(test_losses_fr) + [1] * len(forget_losses_fr)\n",
        "\n",
        "  mia_scores_fr = simple_mia(samples_mia_fr, labels_mia_fr)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr.mean():.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "\n",
        "MIA_scores = [m.mean() for m in MIA_scores]\n",
        "print(MIA_scores)\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)\n",
        "\n",
        "# Mean MIA\n",
        "mean_mia_1 = sum(MIA_scores) / len(MIA_scores)\n",
        "\n",
        "# Mean Accuracy Retain\n",
        "mean_accuracy_retain = sum(Accuracy_retain) / len(Accuracy_retain)\n",
        "mean_accuracy_test = sum(Accuracy_test) / len(Accuracy_test)\n",
        "\n",
        "# Mean Accuracy forget\n",
        "mean_accuracy_forget = sum(Accuracy_forget) / len(Accuracy_forget)\n",
        "\n",
        "print(\"MEANS\")\n",
        "\n",
        "print(mean_mia_1)\n",
        "print(mean_accuracy_retain)\n",
        "print(mean_accuracy_forget)\n",
        "print(mean_accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJYSFgoGYVt6",
        "outputId": "2c74873b-4ca4-477d-f38b-1590833397cb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- 1 PERCENT -------\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.8%\n",
            "Test set accuracy FORGET model: 69.6%\n",
            "Forget set accuracy FORGET model: 90.0%\n",
            "The MIA has an accuracy of 0.620 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.3%\n",
            "Test set accuracy FORGET model: 69.8%\n",
            "Forget set accuracy FORGET model: 92.0%\n",
            "The MIA has an accuracy of 0.610 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.6%\n",
            "Test set accuracy FORGET model: 69.2%\n",
            "Forget set accuracy FORGET model: 74.0%\n",
            "The MIA has an accuracy of 0.540 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 79.9%\n",
            "Test set accuracy FORGET model: 69.2%\n",
            "Forget set accuracy FORGET model: 66.0%\n",
            "The MIA has an accuracy of 0.620 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.4%\n",
            "Test set accuracy FORGET model: 69.5%\n",
            "Forget set accuracy FORGET model: 68.0%\n",
            "The MIA has an accuracy of 0.400 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.3%\n",
            "Test set accuracy FORGET model: 69.4%\n",
            "Forget set accuracy FORGET model: 62.0%\n",
            "The MIA has an accuracy of 0.560 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.5%\n",
            "Test set accuracy FORGET model: 69.4%\n",
            "Forget set accuracy FORGET model: 78.0%\n",
            "The MIA has an accuracy of 0.450 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.4%\n",
            "Test set accuracy FORGET model: 68.8%\n",
            "Forget set accuracy FORGET model: 88.0%\n",
            "The MIA has an accuracy of 0.590 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.4%\n",
            "Test set accuracy FORGET model: 69.5%\n",
            "Forget set accuracy FORGET model: 92.0%\n",
            "The MIA has an accuracy of 0.680 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.2%\n",
            "Test set accuracy FORGET model: 69.6%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.620 on forgotten vs unseen images on FORGET model\n",
            "[0.6200000000000001, 0.6100000000000001, 0.5399999999999999, 0.6199999999999999, 0.4, 0.5599999999999999, 0.45, 0.5900000000000001, 0.6799999999999999, 0.6199999999999999]\n",
            "[80.7671518087726, 81.33133133133134, 80.58858858858858, 79.89429641048227, 81.37337337337337, 81.27727727727728, 81.46146146146145, 80.42642642642642, 81.35535535535536, 81.21997117232543]\n",
            "[90.0, 92.0, 74.0, 66.0, 68.0, 62.0, 78.0, 88.0, 92.0, 86.0]\n",
            "[69.56, 69.77, 69.16, 69.19999999999999, 69.53, 69.42, 69.39, 68.8, 69.45, 69.64]\n",
            "MEANS\n",
            "0.569\n",
            "80.96952332053942\n",
            "79.6\n",
            "69.392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---- 2 PERCANTAGES -----\")\n",
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "MIA_scores = []\n",
        "MIA_scores_retain = []\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "AD_score = []\n",
        "\n",
        "for class_num in classes:\n",
        "  # Define model from trained params\n",
        "\n",
        "  model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "  model_forget_ft.load_state_dict(torch.load(\"model_train_params_best_loss.pt\"))\n",
        "  model_forget_ft.to(DEVICE)\n",
        "\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == class_num)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount = 0.02 # 1 %\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning(model_forget_ft, retain_loader, forget_loader, test_loader)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "  # Retain model\n",
        "  # Add helping dicts\n",
        "  # MIA\n",
        "\n",
        "  # Compute forget losses for forget model\n",
        "  forget_losses_fr = compute_losses(model_ft_forget, forget_loader)\n",
        "  test_losses_fr = compute_losses(model_ft_forget, test_loader)\n",
        "\n",
        "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "  test_losses_fr = test_losses_fr[randomize][: len(forget_losses)]\n",
        "\n",
        "  # make sure we have a balanced dataset for the MIA\n",
        "  samples_mia_fr = np.concatenate((test_losses_fr, forget_losses_fr)).reshape((-1, 1))\n",
        "  labels_mia_fr = [0] * len(test_losses_fr) + [1] * len(forget_losses_fr)\n",
        "\n",
        "  mia_scores_fr = simple_mia(samples_mia_fr, labels_mia_fr)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr.mean():.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "\n",
        "MIA_scores = [m.mean() for m in MIA_scores]\n",
        "print(MIA_scores)\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)\n",
        "\n",
        "# Mean MIA\n",
        "mean_mia_1 = sum(MIA_scores) / len(MIA_scores)\n",
        "\n",
        "# Mean Accuracy Retain\n",
        "mean_accuracy_retain = sum(Accuracy_retain) / len(Accuracy_retain)\n",
        "mean_accuracy_test = sum(Accuracy_test) / len(Accuracy_test)\n",
        "\n",
        "# Mean Accuracy forget\n",
        "mean_accuracy_forget = sum(Accuracy_forget) / len(Accuracy_forget)\n",
        "\n",
        "print(\"MEANS\")\n",
        "\n",
        "print(mean_mia_1)\n",
        "print(mean_accuracy_retain)\n",
        "print(mean_accuracy_forget)\n",
        "print(mean_accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mac5VywIfiQL",
        "outputId": "4896af11-0d7f-4aff-b627-fbf1eeabb591"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- 2 PERCANTAGES -----\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.6%\n",
            "Test set accuracy FORGET model: 69.9%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.713 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.8%\n",
            "Test set accuracy FORGET model: 69.8%\n",
            "Forget set accuracy FORGET model: 88.0%\n",
            "The MIA has an accuracy of 0.653 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.0%\n",
            "Test set accuracy FORGET model: 69.9%\n",
            "Forget set accuracy FORGET model: 70.0%\n",
            "The MIA has an accuracy of 0.667 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.7%\n",
            "Test set accuracy FORGET model: 69.4%\n",
            "Forget set accuracy FORGET model: 60.0%\n",
            "The MIA has an accuracy of 0.667 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.9%\n",
            "Test set accuracy FORGET model: 69.6%\n",
            "Forget set accuracy FORGET model: 73.0%\n",
            "The MIA has an accuracy of 0.667 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.2%\n",
            "Test set accuracy FORGET model: 69.3%\n",
            "Forget set accuracy FORGET model: 63.0%\n",
            "The MIA has an accuracy of 0.660 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.3%\n",
            "Test set accuracy FORGET model: 69.8%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.680 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.4%\n",
            "Test set accuracy FORGET model: 69.3%\n",
            "Forget set accuracy FORGET model: 84.0%\n",
            "The MIA has an accuracy of 0.673 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.1%\n",
            "Test set accuracy FORGET model: 70.3%\n",
            "Forget set accuracy FORGET model: 84.0%\n",
            "The MIA has an accuracy of 0.647 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.7%\n",
            "Test set accuracy FORGET model: 69.6%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.707 on forgotten vs unseen images on FORGET model\n",
            "[0.7133333333333333, 0.6533333333333333, 0.6666666666666667, 0.6666666666666667, 0.6666666666666667, 0.6599999999999999, 0.68, 0.6733333333333332, 0.6466666666666667, 0.7066666666666667]\n",
            "[81.60357507865574, 80.79196809683172, 81.00839662531814, 80.70541082164328, 80.88777555110221, 81.24524067171657, 81.33541741813956, 81.40681362725451, 81.13426853707415, 80.74464461054446]\n",
            "[86.0, 88.0, 70.0, 60.0, 73.0, 63.0, 86.0, 84.0, 84.0, 86.0]\n",
            "[69.94, 69.81, 69.92, 69.44, 69.62, 69.32000000000001, 69.82000000000001, 69.34, 70.3, 69.64]\n",
            "MEANS\n",
            "0.6733333333333333\n",
            "81.08635110382804\n",
            "78.0\n",
            "69.715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---- 5 PERCANTAGES -----\")\n",
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "MIA_scores = []\n",
        "MIA_scores_retain = []\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "AD_score = []\n",
        "\n",
        "for class_num in classes:\n",
        "  # Define model from trained params\n",
        "\n",
        "  model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "  model_forget_ft.load_state_dict(torch.load(\"model_train_params_best_loss.pt\"))\n",
        "  model_forget_ft.to(DEVICE)\n",
        "\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == class_num)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount = 0.05 # 1 %\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning(model_forget_ft, retain_loader, forget_loader, test_loader)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "  # Retain model\n",
        "  # Add helping dicts\n",
        "  # MIA\n",
        "\n",
        "  # Compute forget losses for forget model\n",
        "  forget_losses_fr = compute_losses(model_ft_forget, forget_loader)\n",
        "  test_losses_fr = compute_losses(model_ft_forget, test_loader)\n",
        "\n",
        "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "  test_losses_fr = test_losses_fr[randomize][: len(forget_losses)]\n",
        "\n",
        "  # make sure we have a balanced dataset for the MIA\n",
        "  samples_mia_fr = np.concatenate((test_losses_fr, forget_losses_fr)).reshape((-1, 1))\n",
        "  labels_mia_fr = [0] * len(test_losses_fr) + [1] * len(forget_losses_fr)\n",
        "\n",
        "  mia_scores_fr = simple_mia(samples_mia_fr, labels_mia_fr)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr.mean():.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "\n",
        "MIA_scores = [m.mean() for m in MIA_scores]\n",
        "print(MIA_scores)\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)\n",
        "\n",
        "# Mean MIA\n",
        "mean_mia_1 = sum(MIA_scores) / len(MIA_scores)\n",
        "\n",
        "# Mean Accuracy Retain\n",
        "mean_accuracy_retain = sum(Accuracy_retain) / len(Accuracy_retain)\n",
        "mean_accuracy_test = sum(Accuracy_test) / len(Accuracy_test)\n",
        "\n",
        "# Mean Accuracy forget\n",
        "mean_accuracy_forget = sum(Accuracy_forget) / len(Accuracy_forget)\n",
        "\n",
        "print(\"MEANS\")\n",
        "\n",
        "print(mean_mia_1)\n",
        "print(mean_accuracy_retain)\n",
        "print(mean_accuracy_forget)\n",
        "print(mean_accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQdsk6KTfl9q",
        "outputId": "98467a2d-528c-4e17-e895-2fe4e49448df"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- 5 PERCANTAGES -----\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.5%\n",
            "Test set accuracy FORGET model: 70.6%\n",
            "Forget set accuracy FORGET model: 82.4%\n",
            "The MIA has an accuracy of 0.837 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.9%\n",
            "Test set accuracy FORGET model: 69.6%\n",
            "Forget set accuracy FORGET model: 85.6%\n",
            "The MIA has an accuracy of 0.830 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.4%\n",
            "Test set accuracy FORGET model: 70.1%\n",
            "Forget set accuracy FORGET model: 74.4%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.9%\n",
            "Test set accuracy FORGET model: 69.8%\n",
            "Forget set accuracy FORGET model: 62.4%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.0%\n",
            "Test set accuracy FORGET model: 69.5%\n",
            "Forget set accuracy FORGET model: 70.4%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.4%\n",
            "Test set accuracy FORGET model: 69.5%\n",
            "Forget set accuracy FORGET model: 69.6%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.0%\n",
            "Test set accuracy FORGET model: 69.9%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.1%\n",
            "Test set accuracy FORGET model: 69.2%\n",
            "Forget set accuracy FORGET model: 79.6%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.3%\n",
            "Test set accuracy FORGET model: 69.9%\n",
            "Forget set accuracy FORGET model: 82.8%\n",
            "The MIA has an accuracy of 0.833 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.7%\n",
            "Test set accuracy FORGET model: 69.5%\n",
            "Forget set accuracy FORGET model: 84.8%\n",
            "The MIA has an accuracy of 0.827 on forgotten vs unseen images on FORGET model\n",
            "[0.8366666666666667, 0.8299999999999998, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8266666666666665]\n",
            "[81.50009043953614, 80.90472458350916, 81.36291473242098, 80.91888415467481, 81.0274137792427, 81.3501075182379, 81.00253245970174, 81.09699722635366, 81.25489880820788, 80.73877567426344]\n",
            "[82.39999999999999, 85.6, 74.4, 62.4, 70.39999999999999, 69.6, 86.0, 79.60000000000001, 82.8, 84.8]\n",
            "[70.61, 69.59, 70.12, 69.78, 69.48, 69.47, 69.91000000000001, 69.16, 69.86, 69.5]\n",
            "MEANS\n",
            "0.8326666666666667\n",
            "81.11573393761485\n",
            "77.79999999999998\n",
            "69.74800000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---- 10 % -----\")\n",
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "MIA_scores = []\n",
        "MIA_scores_retain = []\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "AD_score = []\n",
        "\n",
        "for class_num in classes:\n",
        "  # Define model from trained params\n",
        "\n",
        "  model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "  model_forget_ft.load_state_dict(torch.load(\"model_train_params_best_loss.pt\"))\n",
        "  model_forget_ft.to(DEVICE)\n",
        "\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == class_num)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount = 0.1 # 1 %\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning(model_forget_ft, retain_loader, forget_loader, test_loader)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "  # Retain model\n",
        "  # Add helping dicts\n",
        "  # MIA\n",
        "\n",
        "  # Compute forget losses for forget model\n",
        "  forget_losses_fr = compute_losses(model_ft_forget, forget_loader)\n",
        "  test_losses_fr = compute_losses(model_ft_forget, test_loader)\n",
        "\n",
        "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "  test_losses_fr = test_losses_fr[randomize][: len(forget_losses)]\n",
        "\n",
        "  # make sure we have a balanced dataset for the MIA\n",
        "  samples_mia_fr = np.concatenate((test_losses_fr, forget_losses_fr)).reshape((-1, 1))\n",
        "  labels_mia_fr = [0] * len(test_losses_fr) + [1] * len(forget_losses_fr)\n",
        "\n",
        "  mia_scores_fr = simple_mia(samples_mia_fr, labels_mia_fr)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr.mean():.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "\n",
        "MIA_scores = [m.mean() for m in MIA_scores]\n",
        "print(MIA_scores)\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)\n",
        "\n",
        "# Mean MIA\n",
        "mean_mia_1 = sum(MIA_scores) / len(MIA_scores)\n",
        "\n",
        "# Mean Accuracy Retain\n",
        "mean_accuracy_retain = sum(Accuracy_retain) / len(Accuracy_retain)\n",
        "mean_accuracy_test = sum(Accuracy_test) / len(Accuracy_test)\n",
        "\n",
        "# Mean Accuracy forget\n",
        "mean_accuracy_forget = sum(Accuracy_forget) / len(Accuracy_forget)\n",
        "\n",
        "print(\"MEANS\")\n",
        "\n",
        "print(mean_mia_1)\n",
        "print(mean_accuracy_retain)\n",
        "print(mean_accuracy_forget)\n",
        "print(mean_accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bBQIyMflpv8",
        "outputId": "cba2207c-e670-4f81-a207-15f3f9829725"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- 10 % -----\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.1%\n",
            "Test set accuracy FORGET model: 69.8%\n",
            "Forget set accuracy FORGET model: 81.0%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.0%\n",
            "Test set accuracy FORGET model: 69.3%\n",
            "Forget set accuracy FORGET model: 89.4%\n",
            "The MIA has an accuracy of 0.907 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.5%\n",
            "Test set accuracy FORGET model: 69.7%\n",
            "Forget set accuracy FORGET model: 70.4%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.0%\n",
            "Test set accuracy FORGET model: 70.0%\n",
            "Forget set accuracy FORGET model: 60.8%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.6%\n",
            "Test set accuracy FORGET model: 69.7%\n",
            "Forget set accuracy FORGET model: 73.6%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.0%\n",
            "Test set accuracy FORGET model: 69.7%\n",
            "Forget set accuracy FORGET model: 70.6%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.2%\n",
            "Test set accuracy FORGET model: 69.9%\n",
            "Forget set accuracy FORGET model: 86.0%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 81.1%\n",
            "Test set accuracy FORGET model: 69.9%\n",
            "Forget set accuracy FORGET model: 81.0%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.9%\n",
            "Test set accuracy FORGET model: 69.6%\n",
            "Forget set accuracy FORGET model: 85.8%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "Remove Maskconv\n",
            "Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\n",
            "Retain set accuracy FORGET model: 80.8%\n",
            "Test set accuracy FORGET model: 69.5%\n",
            "Forget set accuracy FORGET model: 83.6%\n",
            "The MIA has an accuracy of 0.909 on forgotten vs unseen images on FORGET model\n",
            "[0.909090909090909, 0.907272727272727, 0.909090909090909, 0.909090909090909, 0.909090909090909, 0.909090909090909, 0.909090909090909, 0.909090909090909, 0.909090909090909, 0.909090909090909]\n",
            "[81.14198029397512, 80.98093853485179, 81.51715226038323, 80.99218608031983, 80.63376014864482, 81.028912938136, 81.24217582683842, 81.1436186322612, 80.88392658846332, 80.79843301966802]\n",
            "[81.0, 89.4, 70.39999999999999, 60.8, 73.6, 70.6, 86.0, 81.0, 85.8, 83.6]\n",
            "[69.84, 69.35, 69.74000000000001, 69.95, 69.74000000000001, 69.74000000000001, 69.94, 69.91000000000001, 69.59, 69.49]\n",
            "MEANS\n",
            "0.9089090909090908\n",
            "81.03630843235416\n",
            "78.22\n",
            "69.72900000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalisation algorithm\n",
        "# Calculating score across 10 different classes\n",
        "\n",
        "# Show different classes\n",
        "classes = np.unique(np.array(train_set.targets))\n",
        "\n",
        "MIA_scores = []\n",
        "MIA_scores_retain = []\n",
        "Accuracy_retain = []\n",
        "Accuracy_forget = []\n",
        "Accuracy_test = []\n",
        "AD_score = []\n",
        "# Define model from trained params\n",
        "\n",
        "model_forget_ft = resnet18(weights=None, num_classes=10) # Load resnet18 from pytorch\n",
        "model_forget_ft.load_state_dict(torch.load(\"model_train_params_best_loss.pt\"))\n",
        "model_forget_ft.to(DEVICE)\n",
        "\n",
        "for epoch_num in range(10):\n",
        "\n",
        "\n",
        "  # Choose random forget indecies from some class\n",
        "  # Index of class\n",
        "  class_index = class_num # cars\n",
        "  class_set = np.where(np.array(train_set.targets) == 1)[0]\n",
        "\n",
        "  # Percantage of whole data ( from class )\n",
        "  amount = 0.02 # 2 %\n",
        "  amount_int = class_set.shape[0] * amount\n",
        "\n",
        "  # Get indeces\n",
        "  forget_idx = np.random.choice(class_set, int(amount_int))\n",
        "\n",
        "  # construct indices of retain from those of the forget set\n",
        "  forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "  forget_mask[forget_idx] = True\n",
        "  retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "  # split train set into a forget and a retain set\n",
        "  forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "  retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "  # Generate forget and retain loaders\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  # Unlearn\n",
        "  model_ft_forget = unlearning_by_epochs(model_forget_ft, retain_loader, forget_loader, test_loader, 1)\n",
        "\n",
        "  # Compare accuracy\n",
        "  print(f\"Retain set accuracy FORGET model epoch {epoch_num}: {100.0 * accuracy(model_ft_forget, retain_loader):0.1f}%\")\n",
        "  print(f\"Test set accuracy FORGET model epoch {epoch_num}: {100.0 * accuracy(model_ft_forget, test_loader):0.1f}%\")\n",
        "  print(f\"Forget set accuracy FORGET model {epoch_num}:: {100.0 * accuracy(model_ft_forget, forget_loader):0.1f}%\")\n",
        "  Accuracy_retain.append(100.0 * accuracy(model_ft_forget, retain_loader))\n",
        "  Accuracy_forget.append(100.0 * accuracy(model_ft_forget, forget_loader))\n",
        "  Accuracy_test.append(100.0 * accuracy(model_ft_forget, test_loader))\n",
        "\n",
        "  # Retain model\n",
        "  # Add helping dicts\n",
        "  # MIA\n",
        "\n",
        "  # Compute forget losses for forget model\n",
        "  forget_losses_fr = compute_losses(model_ft_forget, forget_loader)\n",
        "  test_losses_fr = compute_losses(model_ft_forget, test_loader)\n",
        "\n",
        "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
        "  test_losses_fr = test_losses_fr[randomize][: len(forget_losses)]\n",
        "\n",
        "  # make sure we have a balanced dataset for the MIA\n",
        "  samples_mia_fr = np.concatenate((test_losses_fr, forget_losses_fr)).reshape((-1, 1))\n",
        "  labels_mia_fr = [0] * len(test_losses_fr) + [1] * len(forget_losses_fr)\n",
        "\n",
        "  mia_scores_fr = simple_mia(samples_mia_fr, labels_mia_fr)\n",
        "  MIA_scores.append(mia_scores_fr)\n",
        "  print(\n",
        "      f\"The MIA has an accuracy of {mia_scores_fr.mean():.3f} on forgotten vs unseen images on FORGET model\"\n",
        "  )\n",
        "\n",
        "MIA_scores = [m.mean() for m in MIA_scores]\n",
        "print(MIA_scores)\n",
        "print(Accuracy_retain)\n",
        "print(Accuracy_forget)\n",
        "print(Accuracy_test)\n",
        "\n",
        "# Mean MIA\n",
        "mean_mia_1 = sum(MIA_scores) / len(MIA_scores)\n",
        "\n",
        "# Mean Accuracy Retain\n",
        "mean_accuracy_retain = sum(Accuracy_retain) / len(Accuracy_retain)\n",
        "mean_accuracy_test = sum(Accuracy_test) / len(Accuracy_test)\n",
        "\n",
        "# Mean Accuracy forget\n",
        "mean_accuracy_forget = sum(Accuracy_forget) / len(Accuracy_forget)\n",
        "\n",
        "print(\"MEANS\")\n",
        "\n",
        "print(mean_mia_1)\n",
        "print(mean_accuracy_retain)\n",
        "print(mean_accuracy_forget)\n",
        "print(mean_accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvML-PLBqpsq",
        "outputId": "87174b2c-cae1-4abd-8653-b65d7ef0c01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retain set accuracy FORGET model epoch 0: 81.6%\n",
            "Test set accuracy FORGET model epoch 0: 70.8%\n",
            "Forget set accuracy FORGET model 0:: 81.0%\n",
            "The MIA has an accuracy of 0.667 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 1: 85.1%\n",
            "Test set accuracy FORGET model epoch 1: 72.0%\n",
            "Forget set accuracy FORGET model 1:: 95.0%\n",
            "The MIA has an accuracy of 0.767 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 2: 87.0%\n",
            "Test set accuracy FORGET model epoch 2: 71.5%\n",
            "Forget set accuracy FORGET model 2:: 81.0%\n",
            "The MIA has an accuracy of 0.667 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 3: 89.2%\n",
            "Test set accuracy FORGET model epoch 3: 72.9%\n",
            "Forget set accuracy FORGET model 3:: 85.0%\n",
            "The MIA has an accuracy of 0.667 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 4: 87.8%\n",
            "Test set accuracy FORGET model epoch 4: 71.9%\n",
            "Forget set accuracy FORGET model 4:: 83.0%\n",
            "The MIA has an accuracy of 0.640 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 5: 89.8%\n",
            "Test set accuracy FORGET model epoch 5: 72.2%\n",
            "Forget set accuracy FORGET model 5:: 97.0%\n",
            "The MIA has an accuracy of 0.687 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 6: 88.6%\n",
            "Test set accuracy FORGET model epoch 6: 71.3%\n",
            "Forget set accuracy FORGET model 6:: 90.0%\n",
            "The MIA has an accuracy of 0.693 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 7: 88.5%\n",
            "Test set accuracy FORGET model epoch 7: 70.1%\n",
            "Forget set accuracy FORGET model 7:: 93.0%\n",
            "The MIA has an accuracy of 0.707 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 8: 88.7%\n",
            "Test set accuracy FORGET model epoch 8: 71.1%\n",
            "Forget set accuracy FORGET model 8:: 86.0%\n",
            "The MIA has an accuracy of 0.647 on forgotten vs unseen images on FORGET model\n",
            "Retain set accuracy FORGET model epoch 9: 89.2%\n",
            "Test set accuracy FORGET model epoch 9: 70.8%\n",
            "Forget set accuracy FORGET model 9:: 88.0%\n",
            "The MIA has an accuracy of 0.660 on forgotten vs unseen images on FORGET model\n",
            "[0.6666666666666667, 0.7666666666666667, 0.6666666666666667, 0.6666666666666667, 0.6399999999999999, 0.6866666666666666, 0.6933333333333334, 0.7066666666666667, 0.6466666666666667, 0.6599999999999999]\n",
            "[81.64966633935191, 85.10049898799623, 87.01707277973709, 89.24291611558655, 87.84168336673346, 89.7639373171416, 88.60721442885772, 88.48496993987976, 88.71743486973948, 89.24091218788827]\n",
            "[81.0, 95.0, 81.0, 85.0, 83.0, 97.0, 90.0, 93.0, 86.0, 88.0]\n",
            "[70.78, 72.02, 71.55, 72.89999999999999, 71.87, 72.21, 71.26, 70.11, 71.11, 70.83]\n",
            "MEANS\n",
            "0.68\n",
            "87.56663063329121\n",
            "87.9\n",
            "71.464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9F7cNNckviux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}